{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ieiogELf879E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import copy\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import traceback\n",
        "import builtins\n",
        "import itertools\n",
        "from enum import Enum, auto\n",
        "from pathlib import Path\n",
        "from contextlib import contextmanager, redirect_stdout, redirect_stderr\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.axes import Axes\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from scipy.stats import chi2, mannwhitneyu\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "from typing import Iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLQ_XCFL9Apg"
      },
      "outputs": [],
      "source": [
        "ABSTENTION_WEIGHT = 12 # Scaling factor for loss for non-abstention on invalid inputs\n",
        "INCORRECT_ABSTENTION_PENALTY = 25 # Loss penalty for abstention on valid inpouts\n",
        "def set_seed(seed): # Set global seed\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    return seed\n",
        "seed = set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoCtW2KqJzyx"
      },
      "outputs": [],
      "source": [
        "class ArithmeticDataset(Dataset):\n",
        "    def __init__(self, data: List[Dict[str, Any]],\n",
        "                 noise_config: Dict[str, Any] = {'enabled': False, 'std': 0.0}\n",
        "                 ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: Dataset of arithmetic and abstention problems\n",
        "            noise_config: Dict with keys:\n",
        "                - enabled: Boolean to enable/disable noise\n",
        "                - std: Standard deviation of Gaussian noise\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.noise_config = noise_config\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        sample: Dict[str, Any] = self.data[idx]\n",
        "        numbers: Tensor = torch.tensor([sample['Argument 1'], sample['Argument 2']], # Get a sample\n",
        "                                     dtype=torch.float32)\n",
        "\n",
        "        if self.noise_config['enabled']:\n",
        "# Add random noise scaled by the value of 'std' to the dataset, aims to force the model to learn a more complex/difficult decision boundary.\n",
        "            with torch.random.fork_rng():\n",
        "                torch.manual_seed(seed)\n",
        "                noise: Tensor = torch.randn_like(numbers) * self.noise_config['std']\n",
        "                numbers = numbers + noise\n",
        "\n",
        "        # Map operators to indices\n",
        "        op_map: Dict[str, int] = {'+': 0, '-': 1, '@': 2} # @ operator means always abstain, used to test generalization to novel operators.\n",
        "        operator: Tensor = torch.tensor(op_map[sample['Operator']], dtype=torch.long)\n",
        "        result: Tensor = torch.tensor([sample['Result']], dtype=torch.float32)\n",
        "\n",
        "        # Return the tuple of tensors\n",
        "        return numbers, operator, result\n",
        "\n",
        "    @staticmethod\n",
        "    def get_train_val_test_loaders(\n",
        "        dataset_dict: Dict[str, List[Dict[str, Any]]],\n",
        "        batch_size: int = 32,\n",
        "        val_ratio: float = 0.1,\n",
        "        noise_config: Optional[Dict[str, Any]] = None\n",
        "    ) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "        \"\"\"\n",
        "        Create loaders for train, validation, and test sets.\n",
        "        Args:\n",
        "            dataset_dict: Dictionary containing 'train' and 'test' splits of dataset\n",
        "            batch_size: Batch size for dataloaders\n",
        "            val_ratio: Ratio of training data to use for validation\n",
        "            noise_config: Optional noise configuration dictionary\n",
        "        \"\"\"\n",
        "        # Set default noise config if none provided\n",
        "        noise_config = noise_config or {'enabled': False, 'std': 0.0}\n",
        "\n",
        "        # Keep test set separate\n",
        "        test_data: List[Dict[str, Any]] = dataset_dict['test']\n",
        "        train_full: List[Dict[str, Any]] = dataset_dict['train']\n",
        "\n",
        "        # Create stratification keys for training data\n",
        "        stratification_keys: List[str] = []\n",
        "        for item in train_full:\n",
        "            op_type: str = item['Operator']\n",
        "\n",
        "            # Determine validity\n",
        "            if op_type == '@':\n",
        "                category: str = 'invalid'\n",
        "            elif op_type == '+':\n",
        "                sum_result: float = item['Argument 1'] + item['Argument 2']\n",
        "                category = 'overflow' if sum_result > 400 else 'valid' # The category of all of these should maybe be invalid, overflow and underflow don't really add much\n",
        "            elif op_type == '-':\n",
        "                diff_result: float = item['Argument 1'] - item['Argument 2']\n",
        "                category = 'underflow' if diff_result < 0 else 'valid'\n",
        "\n",
        "            strat_key: str = f\"{op_type}_{category}\"\n",
        "            stratification_keys.append(strat_key)\n",
        "\n",
        "        # Get indices for training and validation split\n",
        "        split: StratifiedShuffleSplit = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n",
        "        indices: np.ndarray = np.arange(len(train_full))\n",
        "        train_idx, val_idx = next(split.split(indices, stratification_keys))\n",
        "\n",
        "        # Build datasets using indices\n",
        "        train_data: List[Dict[str, Any]] = [train_full[i] for i in train_idx]\n",
        "        val_data: List[Dict[str, Any]] = [train_full[i] for i in val_idx]\n",
        "\n",
        "        # Create datasets with noise config\n",
        "        train_dataset: ArithmeticDataset = ArithmeticDataset(train_data, noise_config=noise_config)\n",
        "        val_dataset: ArithmeticDataset = ArithmeticDataset(val_data, noise_config=noise_config)\n",
        "        test_dataset: ArithmeticDataset = ArithmeticDataset(test_data, noise_config=noise_config)\n",
        "\n",
        "        # Create loaders\n",
        "        train_loader: DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader: DataLoader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        test_loader: DataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def is_invalid_computation(numbers: Tensor, operator: Tensor) -> Tensor:\n",
        "    \"\"\"Check if computation would be invalid\"\"\"\n",
        "    invalid: Tensor = torch.zeros_like(operator, dtype=torch.bool)\n",
        "\n",
        "    # For addition cases, results that would exceed 400 are invalid\n",
        "    add_mask: Tensor = (operator == 0)\n",
        "    invalid[add_mask] = (numbers[add_mask, 0] + numbers[add_mask, 1] > 400)\n",
        "\n",
        "    # For subtraction cases, results less than 0 are invalid\n",
        "    sub_mask: Tensor = (operator == 1)\n",
        "    invalid[sub_mask] = (numbers[sub_mask, 0] - numbers[sub_mask, 1] < 0)\n",
        "\n",
        "    # The @ operator makes the result automatically invalid\n",
        "    special_mask: Tensor = (operator == 2)\n",
        "    invalid[special_mask] = True\n",
        "\n",
        "    return invalid\n",
        "\n",
        "def create_boundary_test_loader(base_loader, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Creates a test loader with a specific distribution of cases:\n",
        "    - 40% invalid cases (overflow/underflow)\n",
        "    - 40% boundary cases (valid but near limits)\n",
        "    - 20% easy cases (valid and far from boundaries)\n",
        "\n",
        "    Args:\n",
        "        base_loader: Original data loader\n",
        "        num_samples: Number of samples to include\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_invalid_case():\n",
        "        \"\"\"Generate a case that crosses boundaries\"\"\"\n",
        "        op = random.choice(['+', '-'])\n",
        "        if op == '+':\n",
        "            # Generate sum > 400\n",
        "            result = random.randint(401, 500)\n",
        "            arg2 = random.randint(1, 200)\n",
        "            arg1 = result - arg2\n",
        "        else:\n",
        "            # Generate difference < 0\n",
        "            result = random.randint(-100, -1)\n",
        "            arg2 = random.randint(1, 200)\n",
        "            arg1 = result + arg2\n",
        "\n",
        "        return {\n",
        "            'Argument 1': arg1,\n",
        "            'Argument 2': arg2,\n",
        "            'Operator': op,\n",
        "            'Result': result\n",
        "        }\n",
        "\n",
        "    def is_boundary_case(sample):\n",
        "        \"\"\"Check if a valid sample is near the boundary\"\"\"\n",
        "        if sample['Operator'] == '+':\n",
        "            result = sample['Argument 1'] + sample['Argument 2']\n",
        "            return 350 <= result <= 400  # Within 50 of overflow boundary\n",
        "        else:\n",
        "            result = sample['Argument 1'] - sample['Argument 2']\n",
        "            return 0 <= result <= 50  # Within 50 of underflow boundary\n",
        "\n",
        "    def is_valid_case(sample):\n",
        "        \"\"\"Check if a sample is valid (within bounds)\"\"\"\n",
        "        if sample['Operator'] == '+':\n",
        "            return sample['Argument 1'] + sample['Argument 2'] <= 400\n",
        "        else:\n",
        "            return sample['Argument 1'] - sample['Argument 2'] >= 0\n",
        "\n",
        "    # Calculate desired numbers of each type\n",
        "    num_invalid = int(num_samples * 0.4)\n",
        "    num_boundary = int(num_samples * 0.4)\n",
        "    num_easy = num_samples - num_invalid - num_boundary\n",
        "\n",
        "    # Generate invalid cases\n",
        "    invalid_samples = [generate_invalid_case() for _ in range(num_invalid)]\n",
        "\n",
        "    # Collect boundary and easy cases from base loader\n",
        "    boundary_samples = []\n",
        "    easy_samples = []\n",
        "\n",
        "    for idx in range(len(base_loader.dataset.data)):\n",
        "        sample = copy.deepcopy(base_loader.dataset.data[idx])\n",
        "\n",
        "        # Calculate and store result\n",
        "        if sample['Operator'] == '+':\n",
        "            sample['Result'] = sample['Argument 1'] + sample['Argument 2']\n",
        "        else:  # '-'\n",
        "            sample['Result'] = sample['Argument 1'] - sample['Argument 2']\n",
        "\n",
        "        if not is_valid_case(sample):\n",
        "            continue\n",
        "\n",
        "        if is_boundary_case(sample):\n",
        "            if len(boundary_samples) < num_boundary:\n",
        "                boundary_samples.append(sample)\n",
        "        else:\n",
        "            if len(easy_samples) < num_easy:\n",
        "                easy_samples.append(sample)\n",
        "\n",
        "        if (len(boundary_samples) >= num_boundary and\n",
        "            len(easy_samples) >= num_easy):\n",
        "            break\n",
        "\n",
        "    # Handle cases where we don't have enough samples\n",
        "    if len(boundary_samples) < num_boundary:\n",
        "        print(f\"Warning: Only found {len(boundary_samples)} boundary samples\")\n",
        "        missing = num_boundary - len(boundary_samples)\n",
        "        num_easy = num_easy + missing // 2\n",
        "        num_invalid = num_invalid + (missing - missing // 2)\n",
        "\n",
        "    if len(easy_samples) < num_easy:\n",
        "        print(f\"Warning: Only found {len(easy_samples)} easy samples\")\n",
        "        missing = num_easy - len(easy_samples)\n",
        "        num_invalid += missing\n",
        "\n",
        "    # Combine all samples\n",
        "    selected_samples = (\n",
        "        invalid_samples[:num_invalid] +\n",
        "        boundary_samples[:len(boundary_samples)] +\n",
        "        easy_samples[:len(easy_samples)]\n",
        "    )\n",
        "    random.shuffle(selected_samples)\n",
        "\n",
        "    # Create dataset with same noise config as base loader\n",
        "    boundary_dataset = ArithmeticDataset(\n",
        "        data=selected_samples,\n",
        "        noise_config=base_loader.dataset.noise_config\n",
        "    )\n",
        "\n",
        "    # Create and return loader\n",
        "    return DataLoader(\n",
        "        boundary_dataset,\n",
        "        batch_size=base_loader.batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "@contextmanager # Disable printing metrics for faster training\n",
        "def suppress_output():\n",
        "    original_stdout = os.environ.get('PYTHONUNBUFFERED')\n",
        "    original_tqdm = os.environ.get('TQDM_DISABLE')\n",
        "    original_print = builtins.print  # Save the original print function\n",
        "\n",
        "    try:\n",
        "        os.environ['PYTHONUNBUFFERED'] = '0'\n",
        "        os.environ['TQDM_DISABLE'] = '1'\n",
        "\n",
        "        # Create a dummy file-like object that just discards writes\n",
        "        dummy = io.StringIO()\n",
        "        builtins.print = lambda *args, **kwargs: None  # Replace print with no-op\n",
        "        with redirect_stdout(dummy), redirect_stderr(dummy):\n",
        "            yield\n",
        "\n",
        "    finally:\n",
        "        # Restore the environment variables\n",
        "        if original_stdout:\n",
        "            os.environ['PYTHONUNBUFFERED'] = original_stdout\n",
        "        else:\n",
        "            os.environ.pop('PYTHONUNBUFFERED', None)\n",
        "\n",
        "        if original_tqdm:\n",
        "            os.environ['TQDM_DISABLE'] = original_tqdm\n",
        "        else:\n",
        "            os.environ.pop('TQDM_DISABLE', None)\n",
        "\n",
        "        builtins.print = original_print  # Restore the original print function\n",
        "        dummy.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Hi8kfCLSQL"
      },
      "outputs": [],
      "source": [
        "class ArithmeticNet(nn.Module): # Core architecture of the experiment, intentionally very simple to save on compute and minimize complexity.\n",
        "    def __init__(self, hidden_size: int = 128) -> None: # 128 Neurons\n",
        "        super().__init__()\n",
        "\n",
        "        # Input processing\n",
        "        self.num_embedder: nn.Linear = nn.Linear(2, hidden_size)\n",
        "        self.op_embedding: nn.Embedding = nn.Embedding(3, hidden_size)\n",
        "\n",
        "        # Core processing\n",
        "        self.layer1: nn.Linear = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.layer2: nn.Linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3: nn.Linear = nn.Linear(hidden_size, hidden_size//2)\n",
        "\n",
        "        # Single output - no separate abstention head\n",
        "        self.output: nn.Linear = nn.Linear(hidden_size//2, 1)\n",
        "\n",
        "    def forward(self, numbers: Tensor, operator: Tensor) -> Tensor:\n",
        "        # Embed inputs\n",
        "        num_features: Tensor = self.num_embedder(numbers)\n",
        "        op_features: Tensor = self.op_embedding(operator)\n",
        "\n",
        "        # Combine features\n",
        "        x: Tensor = torch.cat([num_features, op_features], dim=1)\n",
        "\n",
        "        # Process with residual connections for better gradient flow\n",
        "        x1: Tensor = F.relu(self.layer1(x))\n",
        "        x2: Tensor = F.relu(self.layer2(x1)) + x1\n",
        "        x3: Tensor = self.layer3(x2)\n",
        "\n",
        "        return self.output(x3)\n",
        "\n",
        "class TrainingMode(Enum):\n",
        "    \"\"\"\n",
        "    Enum defining different training modes.\n",
        "\n",
        "    NORMAL: Standard training with full validation and analysis\n",
        "    FAST: Optimized for speed with reduced validation frequency\n",
        "    HYBRID: Transitions from normal to fast training after initial epochs\n",
        "    \"\"\"\n",
        "    NORMAL = auto()\n",
        "    FAST = auto()\n",
        "    HYBRID = auto()\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"\n",
        "    Unified configuration for managing training parameters and optimization settings.\n",
        "\n",
        "    This class handles both static and dynamic training configurations, supporting\n",
        "    normal, fast, and hybrid training modes. In hybrid mode, it automatically\n",
        "    transitions from normal to fast training after a specified number of epochs.\n",
        "\n",
        "    Attributes:\n",
        "        mode (TrainingMode): Training mode (NORMAL, FAST, or HYBRID)\n",
        "        batch_size (int): Base batch size for training\n",
        "        grad_accum_steps (int): Number of steps for gradient accumulation\n",
        "        val_freq (int): Frequency of validation during training\n",
        "        landscape_freq (int): Frequency of loss landscape analysis\n",
        "        initial_epochs (Optional[int]): Number of initial epochs before switching to fast training in hybrid mode\n",
        "        current_epoch (int): Current training epoch (used for hybrid mode transitions)\n",
        "\n",
        "    Properties:\n",
        "        is_fast_training (bool): Whether fast training optimizations are currently active\n",
        "        effective_batch_size (int): Actual batch size after applying fast training multiplier\n",
        "        effective_grad_accum_steps (int): Actual gradient accumulation steps after mode adjustments\n",
        "        effective_val_freq (int): Actual validation frequency after mode adjustments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        mode: TrainingMode = TrainingMode.NORMAL,\n",
        "        *,\n",
        "        batch_size: int = 32,\n",
        "        grad_accum_steps: int = 4,\n",
        "        val_freq: int = 1,\n",
        "        landscape_freq: int = 40,\n",
        "        initial_epochs: Optional[int] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize training configuration.\n",
        "\n",
        "        Args:\n",
        "            mode: Training mode to use\n",
        "            batch_size: Base batch size\n",
        "            grad_accum_steps: Base gradient accumulation steps\n",
        "            val_freq: Base validation frequency\n",
        "            landscape_freq: Frequency of loss landscape analysis\n",
        "            initial_epochs: Required for HYBRID mode, specifies transition point\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "        self.batch_size = batch_size\n",
        "        self.grad_accum_steps = grad_accum_steps\n",
        "        self.val_freq = val_freq\n",
        "        self.landscape_freq = landscape_freq\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        # Hybrid mode specific settings\n",
        "        if mode == TrainingMode.HYBRID:\n",
        "            if initial_epochs is None:\n",
        "                raise ValueError(\"initial_epochs must be specified for HYBRID mode\")\n",
        "            self.initial_epochs = initial_epochs\n",
        "        else:\n",
        "            self.initial_epochs = None\n",
        "\n",
        "        # Validate configuration\n",
        "        self._validate_config()\n",
        "\n",
        "    def _validate_config(self) -> None:\n",
        "        \"\"\"Validate configuration parameters.\"\"\"\n",
        "        if self.batch_size <= 0:\n",
        "            raise ValueError(\"batch_size must be positive\")\n",
        "        if self.grad_accum_steps <= 0:\n",
        "            raise ValueError(\"grad_accum_steps must be positive\")\n",
        "        if self.val_freq <= 0:\n",
        "            raise ValueError(\"val_freq must be positive\")\n",
        "        if self.landscape_freq <= 0:\n",
        "            raise ValueError(\"landscape_freq must be positive\")\n",
        "\n",
        "    @property\n",
        "    def is_fast_training(self) -> bool:\n",
        "        \"\"\"\n",
        "        Determine if fast training optimizations should be active.\n",
        "\n",
        "        Returns:\n",
        "            True if in FAST mode or if in HYBRID mode past initial epochs\n",
        "        \"\"\"\n",
        "        if self.mode == TrainingMode.FAST:\n",
        "            return True\n",
        "        if self.mode == TrainingMode.HYBRID:\n",
        "            return self.current_epoch >= self.initial_epochs\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def effective_batch_size(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the current effective batch size.\n",
        "\n",
        "        Returns:\n",
        "            Batch size adjusted for fast training mode\n",
        "        \"\"\"\n",
        "        return self.batch_size * 2 if self.is_fast_training else self.batch_size\n",
        "\n",
        "    @property\n",
        "    def effective_grad_accum_steps(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the current effective gradient accumulation steps.\n",
        "\n",
        "        Returns:\n",
        "            Gradient accumulation steps adjusted for current mode\n",
        "        \"\"\"\n",
        "        return self.grad_accum_steps if self.is_fast_training else 1\n",
        "\n",
        "    @property\n",
        "    def effective_val_freq(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the current effective validation frequency.\n",
        "\n",
        "        Returns:\n",
        "            Validation frequency adjusted for fast training mode\n",
        "        \"\"\"\n",
        "        return 2 if self.is_fast_training else self.val_freq\n",
        "\n",
        "    def update_epoch(self, epoch: int) -> None:\n",
        "        \"\"\"\n",
        "        Update the current epoch counter and adjust settings accordingly.\n",
        "\n",
        "        Args:\n",
        "            epoch: New epoch number\n",
        "        \"\"\"\n",
        "        if epoch < 0:\n",
        "            raise ValueError(\"epoch must be non-negative\")\n",
        "        self.current_epoch = epoch\n",
        "\n",
        "    def get_config_dict(self) -> dict:\n",
        "        \"\"\"\n",
        "        Get current configuration as a dictionary.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all current effective settings\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'mode': self.mode.name,\n",
        "            'is_fast_training': self.is_fast_training,\n",
        "            'batch_size': self.effective_batch_size,\n",
        "            'grad_accum_steps': self.effective_grad_accum_steps,\n",
        "            'val_freq': self.effective_val_freq,\n",
        "            'landscape_freq': self.landscape_freq,\n",
        "            'current_epoch': self.current_epoch,\n",
        "            'initial_epochs': self.initial_epochs\n",
        "        }\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Provide a detailed string representation of the configuration.\"\"\"\n",
        "        return (\n",
        "            f\"TrainingConfig(mode={self.mode.name}, \"\n",
        "            f\"is_fast_training={self.is_fast_training}, \"\n",
        "            f\"effective_batch_size={self.effective_batch_size}, \"\n",
        "            f\"effective_grad_accum_steps={self.effective_grad_accum_steps}, \"\n",
        "            f\"effective_val_freq={self.effective_val_freq}, \"\n",
        "            f\"landscape_freq={self.landscape_freq}, \"\n",
        "            f\"current_epoch={self.current_epoch}\"\n",
        "            f\"{f', initial_epochs={self.initial_epochs}' if self.mode == TrainingMode.HYBRID else ''})\"\n",
        "        )\n",
        "class UnifiedLossComputer:\n",
        "    \"\"\"\n",
        "    Unified class for computing losses across training and evaluation.\n",
        "\n",
        "    This class consolidates loss computation logic used by trainers and analyzers,\n",
        "    providing consistent behavior and configuration across the codebase.\n",
        "\n",
        "    Attributes:\n",
        "        -1.0: Value used to indicate model abstention\n",
        "        abstention_weight: Weight applied to abstention loss term\n",
        "        incorrect_abstention_penalty: Penalty for abstaining on valid inputs\n",
        "        rtol: Relative tolerance for abstention detection\n",
        "        atol: Absolute tolerance for abstention detection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        abstention_weight: float = 12.0, # 12, 16, or 25\n",
        "        incorrect_abstention_penalty: float = 25.0, # 25 or 12\n",
        "        rtol: float = 0.1,\n",
        "        atol: float = 0.1\n",
        "    ) -> None:\n",
        "        self.abstention_weight = abstention_weight\n",
        "        self.incorrect_abstention_penalty = incorrect_abstention_penalty\n",
        "        self.rtol = rtol\n",
        "        self.atol = atol\n",
        "\n",
        "    def is_abstention(self, predictions: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Check if predictions indicate abstention.\n",
        "\n",
        "        Args:\n",
        "            predictions: Model output tensor\n",
        "\n",
        "        Returns:\n",
        "            Boolean tensor indicating abstention for each prediction\n",
        "        \"\"\"\n",
        "        return torch.isclose(\n",
        "            predictions.squeeze(),\n",
        "            torch.tensor(-1.0).to(predictions.device),\n",
        "            rtol=self.rtol,\n",
        "            atol=self.atol\n",
        "        )\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        predictions: Tensor,\n",
        "        targets: Tensor,\n",
        "        numbers: Tensor,\n",
        "        operator: Tensor,\n",
        "        reduce: bool = True\n",
        "    ) -> Tuple[Tensor, Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Compute the total loss with abstention incentives and penalties.\n",
        "\n",
        "        Args:\n",
        "            predictions: Model predictions\n",
        "            targets: Ground truth targets\n",
        "            numbers: Input numbers for computation\n",
        "            operator: Operator tokens\n",
        "            reduce: Whether to reduce the loss to a scalar\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (total_loss, component_dict) where component_dict contains:\n",
        "            - valid_loss: Loss on valid computations\n",
        "            - invalid_loss: Loss on invalid computations\n",
        "            - abstention_loss: Penalty for incorrect abstentions\n",
        "        \"\"\"\n",
        "        # Get invalid computation mask\n",
        "        invalid_mask = is_invalid_computation(numbers, operator)\n",
        "        valid_mask = ~invalid_mask\n",
        "\n",
        "        predictions = predictions.squeeze()\n",
        "        targets = targets.squeeze()\n",
        "\n",
        "        components = {}\n",
        "\n",
        "        # Compute MSE loss for valid computations\n",
        "        valid_loss = torch.tensor(0.0, device=predictions.device)\n",
        "        if valid_mask.any():\n",
        "            valid_loss = F.mse_loss(\n",
        "                predictions[valid_mask],\n",
        "                targets[valid_mask],\n",
        "                reduction='mean' if reduce else 'none'\n",
        "            )\n",
        "        components['valid_loss'] = valid_loss.item()\n",
        "\n",
        "        # Compute abstention loss for invalid computations\n",
        "        invalid_loss = torch.tensor(0.0, device=predictions.device)\n",
        "        if invalid_mask.any():\n",
        "            abstention_targets = torch.full_like(\n",
        "                predictions[invalid_mask],\n",
        "                -1.0 # Abstention Token - ! Double Check this\n",
        "            )\n",
        "            invalid_loss = self.abstention_weight * F.mse_loss(\n",
        "                predictions[invalid_mask],\n",
        "                abstention_targets,\n",
        "                reduction='mean' if reduce else 'none'\n",
        "            )\n",
        "        components['invalid_loss'] = invalid_loss.item()\n",
        "\n",
        "        # Combine base losses\n",
        "        total_loss = valid_loss + invalid_loss\n",
        "\n",
        "        # Add penalty for incorrect abstentions on valid computations\n",
        "        abstention_loss = torch.tensor(0.0, device=predictions.device)\n",
        "        if valid_mask.any():\n",
        "            abstained_on_valid = self.is_abstention(predictions[valid_mask])\n",
        "            if abstained_on_valid.any():\n",
        "                num_incorrect = abstained_on_valid.sum().float()\n",
        "                abstention_loss = self.incorrect_abstention_penalty * num_incorrect\n",
        "                if not reduce:\n",
        "                    abstention_loss = abstention_loss.expand_as(predictions)\n",
        "                total_loss = total_loss + abstention_loss\n",
        "        components['abstention_loss'] = abstention_loss.item()\n",
        "\n",
        "        return total_loss, components\n",
        "\n",
        "    def compute_metrics(\n",
        "        self,\n",
        "        predictions: Tensor,\n",
        "        targets: Tensor,\n",
        "        numbers: Tensor,\n",
        "        operator: Tensor\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute comprehensive metrics for model predictions.\n",
        "\n",
        "        Args:\n",
        "            predictions: Model predictions\n",
        "            targets: Ground truth targets\n",
        "            numbers: Input numbers\n",
        "            operator: Operator tokens\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing metrics:\n",
        "            - loss: Total loss value\n",
        "            - valid_accuracy: Accuracy on valid computations\n",
        "            - abstention_rate: Overall abstention rate\n",
        "            - correct_abstentions: Rate of correct abstentions\n",
        "            - incorrect_abstentions: Rate of incorrect abstentions\n",
        "        \"\"\"\n",
        "        invalid_mask = is_invalid_computation(numbers, operator)\n",
        "        valid_mask = ~invalid_mask\n",
        "        abstained = self.is_abstention(predictions)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss, _ = self.compute_loss(predictions, targets, numbers, operator)\n",
        "\n",
        "        metrics = {\n",
        "            'loss': loss.item(),\n",
        "            'abstention_rate': abstained.float().mean().item()\n",
        "        }\n",
        "\n",
        "        # Calculate accuracy on valid predictions\n",
        "        valid_and_not_abstained = valid_mask & (~abstained)\n",
        "        if valid_and_not_abstained.any():\n",
        "            pred_vals = predictions[valid_and_not_abstained]\n",
        "            tgt_vals = targets[valid_and_not_abstained]\n",
        "            diff = torch.abs(pred_vals - tgt_vals)\n",
        "            threshold = torch.abs(tgt_vals) * 0.01 + 1e-8\n",
        "            correct_valid = (diff < threshold).sum().item()\n",
        "            total_valid = valid_and_not_abstained.sum().item()\n",
        "            metrics['valid_accuracy'] = correct_valid / total_valid\n",
        "\n",
        "        # Calculate abstention metrics\n",
        "        if abstained.any():\n",
        "            metrics['correct_abstentions'] = (abstained & invalid_mask).float().mean().item()\n",
        "            metrics['incorrect_abstentions'] = (abstained & valid_mask).float().mean().item()\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YtbJGfZQ_f6"
      },
      "outputs": [],
      "source": [
        "class LandscapeAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzes the loss landscape of neural networks during training, focusing on measures of curvature.\n",
        "\n",
        "    Attributes:\n",
        "        -1.0 (float): Token value used to indicate model abstention\n",
        "        abstention_weight (float): Weight applied to abstention loss term\n",
        "        model (Module): PyTorch model to analyze\n",
        "        alpha (float): Step size for parameter perturbations\n",
        "        num_samples (int): Number of samples to use in analysis\n",
        "        grid_size (int): Resolution of grid for landscape visualization\n",
        "        alpha_range (float): Range of alpha values to explore\n",
        "        save_dir (Optional[Path]): Directory to save analysis results\n",
        "        fixed_scale (float): Fixed scale factor for visualization\n",
        "        metrics_history (defaultdict): History of computed metrics\n",
        "\n",
        "    Args:\n",
        "        model (Module): The neural network model to analyze\n",
        "        alpha (float, optional): Step size for parameter perturbations. Defaults to 0.1\n",
        "        num_samples (int, optional): Number of samples to use in analysis. Defaults to 100\n",
        "        grid_size (int, optional): Resolution of grid for visualization. Defaults to 20\n",
        "        alpha_range (float, optional): Range of alpha values to explore. Defaults to 0.5\n",
        "        save_dir (Optional[str], optional): Directory to save results. Defaults to None\n",
        "        fixed_scale (float, optional): Fixed scale factor for visualization. Defaults to 7\n",
        "        -1.0 (float, optional): Token value for abstention. Defaults to -1.0\n",
        "        abstention_weight (float, optional): Weight for abstention loss. Defaults to 16.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        alpha: float = 0.1,\n",
        "        num_samples: int = 100,\n",
        "        grid_size: int = 20,\n",
        "        alpha_range: float = 0.5,\n",
        "        save_dir: Optional[str] = None,\n",
        "        fixed_scale: float = 7,\n",
        "        abstention_weight: float = ABSTENTION_WEIGHT\n",
        "    ) -> None:\n",
        "        self.abstention_weight = abstention_weight\n",
        "        self.model = model\n",
        "        self.alpha = alpha\n",
        "        self.num_samples = num_samples\n",
        "        self.grid_size = grid_size\n",
        "        self.alpha_range = alpha_range\n",
        "        self.save_dir = Path(save_dir) if save_dir else None\n",
        "        self.fixed_scale = fixed_scale\n",
        "        self.loss_computer = UnifiedLossComputer(\n",
        "            abstention_weight=abstention_weight\n",
        "        )\n",
        "        self.metrics_history: defaultdict = defaultdict(list)\n",
        "\n",
        "    def compute_valley_asymmetry(\n",
        "        self,\n",
        "        batch: Dict[str, Tensor],\n",
        "        num_directions: int = 10\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Measures the asymmetry of loss landscape valleys by comparing loss values\n",
        "        in positive and negative directions from the current parameter position.\n",
        "\n",
        "        The method works by:\n",
        "        1. Storing the original model parameters\n",
        "        2. Sampling random directions in parameter space\n",
        "        3. Computing loss at positive and negative steps in each direction\n",
        "        4. Measuring the asymmetry as the difference between positive and negative deviations\n",
        "\n",
        "        Args:\n",
        "            batch (Dict[str, Tensor]): Input batch containing data and labels\n",
        "            num_directions (int, optional): Number of random directions to sample. Defaults to 10\n",
        "\n",
        "        Returns:\n",
        "            float: Maximum asymmetry found across all sampled directions\n",
        "\n",
        "        Note:\n",
        "            - Higher asymmetry values indicate more irregular loss landscape topology\n",
        "            - The method temporarily modifies model parameters but restores them after computation\n",
        "        \"\"\"\n",
        "        # Store original parameters to restore later\n",
        "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "        original_loss = self.compute_loss(batch)\n",
        "\n",
        "        max_asymmetry = 0\n",
        "        for _ in range(num_directions):\n",
        "            # Sample a random direction in parameter space\n",
        "            direction = self.get_random_direction()\n",
        "\n",
        "            # Measure loss in positive and negative directions\n",
        "            with torch.no_grad():\n",
        "                # Compute loss in positive direction\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.data.copy_(original_params[name] + self.alpha * direction[name])\n",
        "                pos_loss = self.compute_loss(batch)\n",
        "\n",
        "                # Compute loss in negative direction\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.data.copy_(original_params[name] - self.alpha * direction[name])\n",
        "                neg_loss = self.compute_loss(batch)\n",
        "\n",
        "                # Restore original parameters\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.data.copy_(original_params[name])\n",
        "\n",
        "            # Calculate asymmetry as the difference between positive and negative deviations\n",
        "            asymmetry = abs(pos_loss - original_loss - (original_loss - neg_loss))\n",
        "            max_asymmetry = max(max_asymmetry, asymmetry.item())\n",
        "\n",
        "        return max_asymmetry\n",
        "    def compute_alpha_sharpness(self, batch: Tuple[Tensor, Tensor]) -> float:\n",
        "       \"\"\"Compute the α-sharpness measure of the loss landscape by random perturbation sampling.\n",
        "\n",
        "       The α-sharpness is defined as the maximum loss difference when parameters are perturbed\n",
        "       within an α-radius L2 ball:\n",
        "\n",
        "       α-sharpness = max_{||δ||₂ ≤ α} [L(θ + δ) - L(θ)]\n",
        "\n",
        "       where:\n",
        "       - L(θ) is the loss at parameters θ\n",
        "       - δ is the perturbation vector\n",
        "       - α is the perturbation radius\n",
        "\n",
        "       Args:\n",
        "           batch: A tuple of (inputs, targets) tensors representing a batch of data\n",
        "\n",
        "       Returns:\n",
        "           float: The maximum loss difference found across all sampled perturbations\n",
        "\n",
        "       Note:\n",
        "           This implementation approximates the true α-sharpness by random sampling\n",
        "           rather than solving the optimization problem exactly.\n",
        "       \"\"\"\n",
        "       # Store original parameters to restore later\n",
        "       original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "       # Compute loss at original parameters\n",
        "       original_loss = self.compute_loss(batch)\n",
        "\n",
        "       max_loss_diff = 0\n",
        "       # Sample multiple random perturbations to approximate maximum\n",
        "       for _ in range(self.num_samples):\n",
        "           with torch.no_grad():\n",
        "               # Add random perturbation to each parameter, scaled by alpha\n",
        "               for name, param in self.model.named_parameters():\n",
        "                   delta = torch.randn_like(param) * self.alpha  # Gaussian noise\n",
        "                   param.data.add_(delta)\n",
        "\n",
        "               # Compute loss at perturbed parameters\n",
        "               perturbed_loss = self.compute_loss(batch)\n",
        "               # Update maximum loss difference found so far\n",
        "               max_loss_diff = max(max_loss_diff, (perturbed_loss - original_loss).item())\n",
        "\n",
        "               # Restore original parameters for next iteration\n",
        "               for name, param in self.model.named_parameters():\n",
        "                   param.data.copy_(original_params[name])\n",
        "\n",
        "       return max_loss_diff\n",
        "\n",
        "    def compute_multiscale_sharpness(\n",
        "        self,\n",
        "        batch: Tuple[Tensor, Tensor],\n",
        "        scales: List[float] = [0.1, 0.01, 0.001]\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Compute α-sharpness at multiple scale values to analyze loss landscape roughness.\n",
        "\n",
        "        This provides a more complete picture of the loss landscape geometry by measuring\n",
        "        sharpness at different perturbation magnitudes:\n",
        "\n",
        "        {α₁-sharpness, α₂-sharpness, ..., αₙ-sharpness}\n",
        "\n",
        "        Args:\n",
        "            batch: A tuple of (inputs, targets) tensors\n",
        "            scales: List of α values to measure sharpness at, default [0.1, 0.01, 0.001]\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Mapping from scale identifier to sharpness value\n",
        "        \"\"\"\n",
        "        # Store original alpha to restore later\n",
        "        original_alpha = self.alpha\n",
        "        sharpness_values = {}\n",
        "\n",
        "        # Compute sharpness at each scale\n",
        "        for scale in scales:\n",
        "            self.alpha = scale\n",
        "            sharpness_values[f'alpha_{scale}'] = self.compute_alpha_sharpness(batch)\n",
        "\n",
        "        # Restore original alpha\n",
        "        self.alpha = original_alpha\n",
        "        return sharpness_values\n",
        "\n",
        "    def get_hessian_vector_product(\n",
        "        self,\n",
        "        batch: Tuple[Tensor, Tensor],\n",
        "        vector: Tensor,\n",
        "        num_power_iterations: int = 10\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Compute the Hessian-vector product (Hv) using automatic differentiation.\n",
        "\n",
        "        Implements the calculation:\n",
        "        Hv = ∇²L(θ)v = ∇(∇L(θ)ᵀv)\n",
        "\n",
        "        where:\n",
        "        - L(θ) is the loss function\n",
        "        - θ are the model parameters\n",
        "        - v is the input vector\n",
        "        - H is the Hessian matrix\n",
        "\n",
        "        Args:\n",
        "            batch: A tuple of (inputs, targets) tensors\n",
        "            vector: Vector to compute product with\n",
        "            num_power_iterations: Number of power iterations (unused in this implementation)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The Hessian-vector product as a flattened tensor\n",
        "\n",
        "        Note:\n",
        "            This implementation uses automatic differentiation to compute the HVP\n",
        "            without explicitly forming the Hessian matrix.\n",
        "        \"\"\"\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Enable gradient computation for all parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad_(True)\n",
        "\n",
        "        # Forward pass to get loss\n",
        "        loss = self.compute_loss(batch)\n",
        "\n",
        "        # Compute first-order gradients\n",
        "        grads = torch.autograd.grad(\n",
        "            loss,\n",
        "            self.model.parameters(),\n",
        "            allow_unused=True,  # Some parameters might not influence loss\n",
        "            create_graph=True   # Enable second derivative computation\n",
        "        )\n",
        "\n",
        "        # Replace None gradients with zeros for unused parameters\n",
        "        grads = [torch.zeros_like(p) if g is None else g\n",
        "                 for g, p in zip(grads, self.model.parameters())]\n",
        "\n",
        "        # Compute gradient-vector product (first term in chain rule)\n",
        "        flat_grad = torch.cat([g.flatten() for g in grads])\n",
        "        grad_vector_product = torch.dot(flat_grad, vector)\n",
        "\n",
        "        # Compute Hessian-vector product via second backward pass\n",
        "        hvp = torch.autograd.grad(\n",
        "            grad_vector_product,\n",
        "            self.model.parameters(),\n",
        "            allow_unused=True\n",
        "        )\n",
        "\n",
        "        # Replace None values with zeros for unused parameters\n",
        "        hvp = [torch.zeros_like(p) if h is None else h\n",
        "               for h, p in zip(hvp, self.model.parameters())]\n",
        "\n",
        "        # Return flattened HVP\n",
        "        return torch.cat([h.flatten() for h in hvp])\n",
        "\n",
        "    def estimate_top_k_eigenvalues(\n",
        "        self,\n",
        "        batch: Tuple[Tensor, Tensor, Tensor],\n",
        "        k: int = 3,\n",
        "        num_power_iterations: int = 10\n",
        "    ) -> List[float]:\n",
        "       \"\"\"Estimate the top k eigenvalues of the Hessian matrix using power iteration.\n",
        "\n",
        "       Uses the power iteration method with deflation to find the largest eigenvalues\n",
        "       of the Hessian matrix. The method iteratively computes:\n",
        "\n",
        "       v_{t+1} = Hv_t / ||Hv_t||\n",
        "       λ = v^T Hv\n",
        "\n",
        "       where:\n",
        "       - H is the Hessian matrix\n",
        "       - v_t is the estimate of eigenvector at iteration t\n",
        "       - λ is the corresponding eigenvalue\n",
        "\n",
        "       Args:\n",
        "           batch: Tuple of (numbers, operator, targets) tensors\n",
        "           k: Number of top eigenvalues to estimate\n",
        "           num_power_iterations: Number of power iterations for each eigenvalue\n",
        "\n",
        "       Returns:\n",
        "           List[float]: Top k eigenvalues of the Hessian in descending order\n",
        "\n",
        "       Note:\n",
        "           Uses deflation to find subsequent eigenvalues by removing projections\n",
        "           onto previously found eigenvectors.\n",
        "       \"\"\"\n",
        "       device = next(self.model.parameters()).device\n",
        "       eigenvalues = []\n",
        "       eigenvectors = []\n",
        "\n",
        "       for i in range(k):\n",
        "           # Initialize random vector with fixed seed for reproducibility\n",
        "           with torch.random.fork_rng():\n",
        "               torch.manual_seed(seed)\n",
        "               vector = torch.randn(sum(p.numel() for p in self.model.parameters())).to(device)\n",
        "               vector = vector / torch.norm(vector)  # Normalize to unit vector\n",
        "\n",
        "           # Power iteration\n",
        "           for _ in range(num_power_iterations):\n",
        "               vector_new = self.get_hessian_vector_product(batch, vector)\n",
        "\n",
        "               # Deflate: Remove projections onto previous eigenvectors\n",
        "               for prev_vec in eigenvectors:\n",
        "                   vector_new = vector_new - torch.dot(vector_new, prev_vec) * prev_vec\n",
        "\n",
        "               # Normalize the vector, checking for numerical stability\n",
        "               norm = torch.norm(vector_new)\n",
        "               if norm > 1e-10:\n",
        "                   vector = vector_new / norm\n",
        "               else:\n",
        "                   print(\"\\nWarning: Near-zero vector in power iteration\")\n",
        "                   break\n",
        "\n",
        "           # Compute Rayleigh quotient to get eigenvalue\n",
        "           hvp = self.get_hessian_vector_product(batch, vector)\n",
        "           eigenvalue = torch.dot(vector, hvp)\n",
        "\n",
        "           # Handle numerical instability\n",
        "           if torch.isnan(eigenvalue):\n",
        "               print(f\"\\nWarning: NaN eigenvalue detected for eigenvector {i+1}\")\n",
        "               eigenvalue = torch.tensor(0.0)\n",
        "\n",
        "           eigenvalues.append(eigenvalue.item())\n",
        "           eigenvectors.append(vector)\n",
        "\n",
        "       return eigenvalues\n",
        "\n",
        "    def analyze_landscape(\n",
        "        self,\n",
        "        batch: Tuple[Tensor, Tensor, Tensor],\n",
        "        epoch: Optional[int] = None\n",
        "    ) -> Dict[str, Union[float, List[float], Dict[str, float]]]:\n",
        "        \"\"\"Analyze various geometric properties of the loss landscape.\n",
        "\n",
        "        Computes multiple metrics to characterize the loss landscape geometry:\n",
        "        1. Valley asymmetry: Measures asymmetric properties of loss valleys\n",
        "        2. Top eigenvalues: Largest eigenvalues of the Hessian\n",
        "        3. Multiscale sharpness: Loss variation at different perturbation scales\n",
        "        4. Alpha sharpness: Maximum loss variation within alpha-radius ball\n",
        "\n",
        "        Args:\n",
        "            batch: Tuple of (numbers, operator, targets) tensors\n",
        "            epoch: Optional epoch number for saving metrics\n",
        "\n",
        "        Returns:\n",
        "            Dict containing computed landscape metrics:\n",
        "                - 'valley_asymmetry': float\n",
        "                - 'top_eigenvalues': List[float]\n",
        "                - 'multiscale_sharpness': Dict[str, float]\n",
        "                - 'alpha_sharpness': float\n",
        "\n",
        "        Note:\n",
        "            If epoch is provided, metrics are saved to a JSON file.\n",
        "        \"\"\"\n",
        "        # Compute individual landscape metrics\n",
        "        valley_asymmetry = self.compute_valley_asymmetry(batch)\n",
        "        top_eigenvalues = self.estimate_top_k_eigenvalues(batch, k=3)\n",
        "        multiscale_sharpness = self.compute_multiscale_sharpness(batch)\n",
        "        alpha_sharpness = self.compute_alpha_sharpness(batch)\n",
        "\n",
        "        # Aggregate metrics\n",
        "        metrics = {\n",
        "            'valley_asymmetry': valley_asymmetry,\n",
        "            'top_eigenvalues': top_eigenvalues,\n",
        "            'multiscale_sharpness': multiscale_sharpness,\n",
        "            'alpha_sharpness': alpha_sharpness\n",
        "        }\n",
        "\n",
        "        # Save metrics if epoch is provided\n",
        "        if epoch is not None:\n",
        "            save_path = os.path.join(self.save_dir, f\"landscape_metrics_epoch_{epoch}.json\")\n",
        "            with open(save_path, 'w') as f:\n",
        "                json.dump(metrics, f)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        batch: Tuple[Tensor, Tensor, Tensor]\n",
        "    ) -> Tensor:\n",
        "        numbers, operator, targets = batch\n",
        "        predictions = self.model(numbers, operator)\n",
        "        loss, _ = self.loss_computer.compute_loss(predictions, targets, numbers, operator)\n",
        "        return loss\n",
        "    def evaluate_loss_surface(\n",
        "            self,\n",
        "            data_loader: DataLoader,\n",
        "            dir1: Dict[str, Tensor],\n",
        "            dir2: Dict[str, Tensor]\n",
        "        ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Evaluate the loss on a 2D grid in parameter space along two directions.\n",
        "\n",
        "        Computes the loss surface by perturbing model parameters along two directions:\n",
        "        θ(α,β) = θ₀ + α·d₁ + β·d₂\n",
        "\n",
        "        where:\n",
        "        - θ₀ is the original parameter vector\n",
        "        - d₁, d₂ are the perturbation directions\n",
        "        - α, β are scaling factors\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader providing batches of training data\n",
        "            dir1: First perturbation direction as {param_name: direction_tensor}\n",
        "            dir2: Second perturbation direction as {param_name: direction_tensor}\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - alphas: 1D array of α values\n",
        "            - betas: 1D array of β values\n",
        "            - loss_surface: 2D array of loss values at each (α,β) point\n",
        "        \"\"\"\n",
        "        # Store original parameters to restore later\n",
        "        original_params = {\n",
        "            name: param.data.clone()\n",
        "            for name, param in self.model.named_parameters()\n",
        "        }\n",
        "\n",
        "        # Create grid of perturbation scales\n",
        "        alphas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        betas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        loss_surface = np.zeros((self.grid_size, self.grid_size))\n",
        "\n",
        "        # Evaluate loss at each grid point\n",
        "        for i, alpha in enumerate(alphas):\n",
        "            for j, beta in enumerate(betas):\n",
        "                with torch.no_grad():\n",
        "                    # Apply perturbation: θ = θ₀ + α·d₁ + β·d₂\n",
        "                    for name, param in self.model.named_parameters():\n",
        "                        param.data.copy_(\n",
        "                            original_params[name] + alpha * dir1[name] + beta * dir2[name]\n",
        "                        )\n",
        "\n",
        "                    # Average loss over multiple batches\n",
        "                    total_loss = sum(\n",
        "                        self.compute_loss(batch).item()\n",
        "                        for batch in itertools.islice(data_loader, 5)\n",
        "                    )\n",
        "                    loss_surface[i, j] = total_loss / 5\n",
        "\n",
        "            if i % 5 == 0:\n",
        "                print(f\"Completed {i + 1}/{self.grid_size} rows\")\n",
        "\n",
        "        # Restore original parameters\n",
        "        for name, param in self.model.named_parameters():\n",
        "            param.data.copy_(original_params[name])\n",
        "\n",
        "        return alphas, betas, loss_surface\n",
        "\n",
        "\n",
        "    def evaluate_landscape_with_abstention(\n",
        "        self,\n",
        "        data_loader: DataLoader,\n",
        "        dir1: Dict[str, Tensor],\n",
        "        dir2: Dict[str, Tensor]\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Evaluate loss landscape with abstention metrics along two parameter directions.\n",
        "\n",
        "        Computes multiple surfaces characterizing model behavior:\n",
        "        1. Loss surface\n",
        "        2. Abstention rates\n",
        "        3. Valid accuracy\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader providing batches of training data\n",
        "            dir1: First perturbation direction as {param_name: direction_tensor}\n",
        "            dir2: Second perturbation direction as {param_name: direction_tensor}\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - alphas: 1D array of α values\n",
        "            - betas: 1D array of β values\n",
        "            - loss_surface: 2D array of loss values\n",
        "            - abstention_rates: 2D array of abstention rates\n",
        "            - valid_accuracy: 2D array of accuracy on valid inputs\n",
        "        \"\"\"\n",
        "        original_params = {name: param.data.clone()\n",
        "                          for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Create evaluation grid\n",
        "        alphas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        betas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "\n",
        "        # Initialize metric surfaces\n",
        "        loss_surface = np.zeros((self.grid_size, self.grid_size))\n",
        "        abstention_rates = np.zeros((self.grid_size, self.grid_size))\n",
        "        valid_accuracy = np.zeros((self.grid_size, self.grid_size))\n",
        "\n",
        "        for i, alpha in enumerate(alphas):\n",
        "            for j, beta in enumerate(betas):\n",
        "                with torch.no_grad():\n",
        "                    # Apply parameter perturbation\n",
        "                    for name, param in self.model.named_parameters():\n",
        "                        param.data.copy_(original_params[name] +\n",
        "                                       alpha * dir1[name] +\n",
        "                                       beta * dir2[name])\n",
        "\n",
        "                    # Initialize metrics for current point\n",
        "                    total_loss = 0.0\n",
        "                    total_abstentions = 0\n",
        "                    total_valid = 0\n",
        "                    total_correct = 0\n",
        "\n",
        "                    for batch in itertools.islice(data_loader, 5):\n",
        "                        numbers, operator, targets = batch\n",
        "                        predictions = self.model(numbers, operator)\n",
        "\n",
        "                        # Compute metrics\n",
        "                        loss = self.compute_loss(batch)\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                        predictions = predictions.squeeze()\n",
        "                        # Identify abstentions\n",
        "                        abstained = torch.isclose(\n",
        "                            predictions,\n",
        "                            torch.tensor(-1.0, device=predictions.device),\n",
        "                            rtol=0.1, atol=0.1\n",
        "                        )\n",
        "\n",
        "                        # Compute accuracy on valid, non-abstained predictions\n",
        "                        invalid_mask = is_invalid_computation(numbers, operator)\n",
        "                        valid_mask = ~invalid_mask\n",
        "                        valid_and_not_abstained = valid_mask & (~abstained)\n",
        "\n",
        "                        if valid_and_not_abstained.any():\n",
        "                            pred_vals = predictions[valid_and_not_abstained]\n",
        "                            tgt_vals = targets[valid_and_not_abstained].squeeze()\n",
        "                            diff = torch.abs(pred_vals - tgt_vals)\n",
        "                            threshold = torch.abs(tgt_vals) * 0.01 + 1e-8\n",
        "                            correct_valid = (diff < threshold).sum().item()\n",
        "                            total_correct += correct_valid\n",
        "\n",
        "                        total_abstentions += abstained.sum().item()\n",
        "                        total_valid += len(predictions)\n",
        "\n",
        "                    # Update metric surfaces\n",
        "                    loss_surface[i, j] = total_loss / 5\n",
        "                    abstention_rates[i, j] = total_abstentions / total_valid\n",
        "                    valid_accuracy[i, j] = total_correct / total_valid\n",
        "\n",
        "        # Restore original parameters\n",
        "        for name, param in self.model.named_parameters():\n",
        "            param.data.copy_(original_params[name])\n",
        "\n",
        "        return alphas, betas, loss_surface, abstention_rates, valid_accuracy\n",
        "\n",
        "    def visualize_landscape(\n",
        "        self,\n",
        "        data_loader: DataLoader,\n",
        "        epoch: Optional[int] = None,\n",
        "        model_name: Optional[str] = None,\n",
        "        rand_dir: bool = True\n",
        "        ) -> None:\n",
        "        \"\"\"Visualize multiple aspects of the loss landscape in 3D.\n",
        "\n",
        "        Creates a three-panel visualization showing:\n",
        "        1. Local Lipschitz constants\n",
        "        2. Distance to decision boundary\n",
        "        3. Loss surface (log scale)\n",
        "\n",
        "        Each surface is plotted along two directions in parameter space,\n",
        "        either random or principal directions based on the Hessian.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader providing batches of training data\n",
        "            epoch: Optional epoch number for saving visualization\n",
        "            model_name: Optional model name for saving visualization\n",
        "            rand_dir: Whether to use random directions (True) or\n",
        "                     principal directions (False)\n",
        "        \"\"\"\n",
        "        # Get perturbation directions\n",
        "        if rand_dir:\n",
        "            dir1 = self.get_random_direction()\n",
        "            dir2 = self.get_random_direction()\n",
        "        else:\n",
        "            dir1, dir2 = self.get_principal_directions(next(iter(data_loader)))\n",
        "\n",
        "        # Create coordinate grid\n",
        "        alphas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        betas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        alpha_grid, beta_grid = np.meshgrid(alphas, betas)\n",
        "\n",
        "        # Compute surfaces\n",
        "        print(\"Computing Lipschitz surface...\")\n",
        "        _, _, llc_surface = self.evaluate_lipschitz_surface(data_loader, dir1, dir2)\n",
        "\n",
        "        print(\"Computing Distance to Decision Boundary surface...\")\n",
        "        _, _, ddb_surface = self.evaluate_distance_to_boundary(data_loader, dir1, dir2)\n",
        "\n",
        "        print(\"Computing loss surface...\")\n",
        "        _, _, loss_surface = self.evaluate_loss_surface(data_loader, dir1, dir2)\n",
        "\n",
        "        # Create visualization\n",
        "        fig = plt.figure(figsize=(20, 7))\n",
        "        gs = plt.GridSpec(1, 3, width_ratios=[1, 1, 1], wspace=0.2)\n",
        "\n",
        "        # Plot 1: Local Lipschitz Constants\n",
        "        ax1 = fig.add_subplot(gs[0], projection='3d')\n",
        "        llc_surface_smooth = gaussian_filter(llc_surface, sigma=1.0)\n",
        "        surf1 = ax1.plot_surface(alpha_grid, beta_grid, llc_surface_smooth,\n",
        "                               cmap='coolwarm', antialiased=True)\n",
        "        ax1.grid(True, linestyle='--', alpha=0.3)\n",
        "        ax1.set_title(\"Local Lipschitz Constants\\n(Smoothed)\")\n",
        "        fig.colorbar(surf1, ax=ax1, pad=0.12, label=\"LLC Magnitude\")\n",
        "\n",
        "        # Plot 2: Distance to Decision Boundary\n",
        "        ax2 = fig.add_subplot(gs[1], projection='3d')\n",
        "        ddb_surface_smooth = gaussian_filter(ddb_surface, sigma=1.0)\n",
        "        surf2 = ax2.plot_surface(alpha_grid, beta_grid, ddb_surface_smooth,\n",
        "                               cmap='plasma', antialiased=True)\n",
        "        ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "        ax2.set_title(\"Distance to Decision Boundary\\n(Smoothed)\")\n",
        "        fig.colorbar(surf2, ax=ax2, pad=0.12, label=\"Distance\")\n",
        "\n",
        "        # Plot 3: Loss Surface (log scale)\n",
        "        ax3 = fig.add_subplot(gs[2], projection='3d')\n",
        "        loss_surface_log = np.log1p(loss_surface)\n",
        "        loss_surface_smooth = gaussian_filter(loss_surface_log, sigma=1.0)\n",
        "        surf3 = ax3.plot_surface(alpha_grid, beta_grid, loss_surface_smooth,\n",
        "                               cmap='magma', antialiased=True)\n",
        "        ax3.grid(True, linestyle='--', alpha=0.3)\n",
        "        ax3.set_title(\"Loss Landscape\\n(Log Scale, Smoothed)\")\n",
        "        fig.colorbar(surf3, ax=ax3, pad=0.12, label=\"Log Loss\")\n",
        "\n",
        "        # Style all axes consistently\n",
        "        for ax in [ax1, ax2, ax3]:\n",
        "            ax.set_xlabel('α')\n",
        "            ax.set_ylabel('β')\n",
        "            ax.view_init(elev=20, azim=45)\n",
        "            ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "            ax.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "            ax.zaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "            ax.xaxis._axinfo[\"grid\"]['color'] = (1,1,1,0.2)\n",
        "            ax.yaxis._axinfo[\"grid\"]['color'] = (1,1,1,0.2)\n",
        "            ax.zaxis._axinfo[\"grid\"]['color'] = (1,1,1,0.2)\n",
        "\n",
        "        # Save visualization if requested\n",
        "        if self.save_dir and epoch is not None and model_name:\n",
        "            save_path = os.path.join(self.save_dir,\n",
        "                                    f\"landscape_epoch_{epoch}_{model_name}.png\")\n",
        "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "            print(f\"Saved landscape visualization to {save_path}\")\n",
        "        plt.close()\n",
        "    def get_principal_directions(\n",
        "       self,\n",
        "       batch: Tuple[Tensor, ...],\n",
        "       k: int = 2\n",
        "       ) -> Tuple[Dict[str, Tensor], Dict[str, Tensor]]:\n",
        "       \"\"\"Compute the top k principal directions of the Hessian using power iteration.\n",
        "       Uses modified power iteration to find the eigenvectors corresponding to the\n",
        "       largest eigenvalues of the Hessian matrix. Implements deflation to find\n",
        "       multiple orthogonal directions.\n",
        "       The algorithm iteratively computes:\n",
        "       v_{t+1} = Hv_t / ||Hv_t||\n",
        "       where H is the Hessian matrix and v_t is the current estimate of an eigenvector.\n",
        "       Args:\n",
        "           batch: Tuple of input tensors for computing Hessian\n",
        "           k: Number of principal directions to compute (default: 2)\n",
        "       Returns:\n",
        "           Tuple of two dictionaries mapping parameter names to direction tensors,\n",
        "           representing the top two principal directions\n",
        "       Note:\n",
        "           Handles potential numerical instabilities with zero vectors\n",
        "           that can occur with unused parameters.\n",
        "       \"\"\"\n",
        "       device = next(self.model.parameters()).device\n",
        "       directions = {}\n",
        "       vectors = []  # Store flat vectors for deflation\n",
        "       for i in range(k):\n",
        "           # Initialize random vector\n",
        "           with torch.random.fork_rng():\n",
        "               torch.manual_seed(seed)\n",
        "               vector = torch.randn(sum(p.numel() for p in self.model.parameters())).to(device)\n",
        "               vector = vector / torch.norm(vector)\n",
        "            # Power iteration\n",
        "           for _ in range(10):\n",
        "               vector_new = self.get_hessian_vector_product(batch, vector)\n",
        "               # Deflate against previous eigenvectors\n",
        "               for prev_vec in vectors:\n",
        "                   vector_new = vector_new - torch.dot(vector_new, prev_vec) * prev_vec\n",
        "               # Handle numerical stability\n",
        "               if torch.norm(vector_new) > 1e-10:\n",
        "                   vector = vector_new / torch.norm(vector_new)\n",
        "               else:\n",
        "                   # Reset to random if we hit a zero vector\n",
        "                   vector = torch.randn_like(vector)\n",
        "                   vector = vector / torch.norm(vector)\n",
        "           vectors.append(vector)\n",
        "           # Convert flat vector back to parameter dictionary\n",
        "           direction = {}\n",
        "           offset = 0\n",
        "           for name, param in self.model.named_parameters():\n",
        "               numel = param.numel()\n",
        "               direction[name] = vector[offset:offset+numel].reshape(param.shape)\n",
        "               offset += numel\n",
        "           directions[f'pc{i+1}'] = direction\n",
        "       return directions['pc1'], directions['pc2']\n",
        "    def get_random_direction(self) -> Dict[str, Tensor]:\n",
        "        \"\"\"Generate a normalized random direction in parameter space.\n",
        "          Creates a dictionary mapping parameter names to random direction tensors,\n",
        "        where each direction tensor has the same shape as its corresponding parameter\n",
        "        and is normalized to unit length.\n",
        "          Returns:\n",
        "            Dict mapping parameter names to normalized random direction tensors\n",
        "        \"\"\"\n",
        "        direction = {}\n",
        "        with torch.random.fork_rng():\n",
        "            torch.manual_seed(seed)\n",
        "            for name, param in self.model.named_parameters():\n",
        "                direction[name] = torch.randn_like(param)\n",
        "                direction[name] /= torch.norm(direction[name])\n",
        "        return direction\n",
        "    def compute_local_lipschitz(\n",
        "      self,\n",
        "      batch: Tuple[Tensor, ...],\n",
        "      point: Tuple[float, float],\n",
        "      radius: float = 0.1,\n",
        "      num_samples: int = 100\n",
        "      ) -> float:\n",
        "      \"\"\"Compute the local Lipschitz constant in a neighborhood of a point.\n",
        "        The local Lipschitz constant L is computed as:\n",
        "      L = max_{x,y in B(point,radius)} ||f(x) - f(y)|| / ||x - y||\n",
        "        where:\n",
        "      - B(point,radius) is the ball of given radius around the point\n",
        "      - f is the model function\n",
        "      - ||·|| denotes appropriate norms\n",
        "        Args:\n",
        "          batch: Input data batch\n",
        "          point: Point in parameter space to compute Lipschitz constant around\n",
        "          radius: Radius of neighborhood to sample in\n",
        "            num_samples: Number of random directions to sample\n",
        "\n",
        "        Returns:\n",
        "            float: Estimated local Lipschitz constant\n",
        "        \"\"\"\n",
        "      original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "      device = next(self.model.parameters()).device\n",
        "      f_0 = self.model(*batch[:2])\n",
        "      max_lipschitz = 0\n",
        "      # Sample random directions to estimate Lipschitz constant\n",
        "      for _ in range(num_samples):\n",
        "          direction = self.get_random_direction()\n",
        "          scale = torch.rand(1).item() * radius\n",
        "          # Compute output at perturbed point\n",
        "          with torch.no_grad():\n",
        "              for name, param in self.model.named_parameters():\n",
        "                  param.data.copy_(original_params[name] + scale * direction[name])\n",
        "              # Compute Lipschitz quotient\n",
        "              f_x = self.model(*batch[:2])\n",
        "              output_diff = torch.norm(f_x - f_0)\n",
        "              output_diff = torch.norm(f_x - f_0)\n",
        "              param_diff_squared = sum(torch.norm(d).item() ** 2 for d in direction.values())\n",
        "              param_diff = scale * np.sqrt(param_diff_squared)\n",
        "              lipschitz = output_diff.item() / param_diff\n",
        "              max_lipschitz = max(max_lipschitz, lipschitz)\n",
        "        # Restore original parameters\n",
        "      for name, param in self.model.named_parameters():\n",
        "          param.data.copy_(original_params[name])\n",
        "      return max_lipschitz\n",
        "\n",
        "    def evaluate_lipschitz_surface(\n",
        "        self,\n",
        "        data_loader: DataLoader,\n",
        "        dir1: Dict[str, Tensor],\n",
        "        dir2: Dict[str, Tensor]\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Evaluate local Lipschitz constants across a 2D slice of parameter space.\n",
        "        Computes a grid of local Lipschitz constants by perturbing parameters along\n",
        "        two specified directions:\n",
        "        θ(α,β) = θ₀ + α·d₁ + β·d₂\n",
        "        Args:\n",
        "            data_loader: DataLoader providing training batches\n",
        "            dir1: First perturbation direction\n",
        "            dir2: Second perturbation direction\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - alphas: 1D array of α values\n",
        "            - betas: 1D array of β values\n",
        "            - llc_surface: 2D array of local Lipschitz constants\n",
        "        \"\"\"\n",
        "        original_params = {name: param.data.clone() for name, param in self.model.named_parameters()}\n",
        "        alphas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        betas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        llc_surface = np.zeros((self.grid_size, self.grid_size))\n",
        "        for i, alpha in enumerate(alphas):\n",
        "            for j, beta in enumerate(betas):\n",
        "                # Move to grid point\n",
        "                with torch.no_grad():\n",
        "                    for name, param in self.model.named_parameters():\n",
        "                        param.data.copy_(original_params[name] +\n",
        "                                       alpha * dir1[name] +\n",
        "                                       beta * dir2[name])\n",
        "                # Compute LLC at current point\n",
        "                batch = next(iter(data_loader))\n",
        "                llc_surface[i, j] = self.compute_local_lipschitz(batch, (alpha, beta))\n",
        "            if i % 5 == 0:\n",
        "                print(f\"Completed {i + 1}/{self.grid_size} LLC rows\")\n",
        "        # Restore original parameters\n",
        "        for name, param in self.model.named_parameters():\n",
        "            param.data.copy_(original_params[name])\n",
        "        return alphas, betas, llc_surface\n",
        "\n",
        "    def evaluate_distance_to_boundary(\n",
        "        self,\n",
        "        data_loader: DataLoader,\n",
        "        dir1: Dict[str, Tensor],\n",
        "        dir2: Dict[str, Tensor]\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Evaluate distance to decision boundary across a 2D parameter slice.\n",
        "        Uses binary search to find the minimum distance to a decision boundary\n",
        "        (where model behavior changes) at each point in a grid.\n",
        "        Args:\n",
        "            data_loader: DataLoader providing training batches\n",
        "            dir1: First perturbation direction\n",
        "            dir2: Second perturbation direction\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - alphas: 1D array of α values\n",
        "            - betas: 1D array of β values\n",
        "            - distance_surface: 2D array of distances to nearest boundary\n",
        "        \"\"\"\n",
        "        device = next(self.model.parameters()).device\n",
        "        original_params = {name: param.data.clone() for name, param in self.model.named_parameters()}\n",
        "        # Setup evaluation grid\n",
        "        alphas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        betas = np.linspace(-self.alpha_range, self.alpha_range, self.grid_size)\n",
        "        distance_surface = np.zeros((self.grid_size, self.grid_size))\n",
        "        # Get test batch near decision boundaries\n",
        "        boundary_loader = create_boundary_test_loader(data_loader, num_samples=100)\n",
        "        test_batch = next(iter(boundary_loader))\n",
        "        # Evaluate grid points\n",
        "        for i, alpha in enumerate(alphas):\n",
        "            for j, beta in enumerate(betas):\n",
        "                with torch.no_grad():\n",
        "                    # Set parameters for current grid point\n",
        "                    for name, param in self.model.named_parameters():\n",
        "                        new_param = (original_params[name] +\n",
        "                                   alpha * dir1[name] +\n",
        "                                   beta * dir2[name])\n",
        "                        param.data.copy_(new_param)\n",
        "\n",
        "                    # Get base behavior\n",
        "                    numbers, operator, targets = test_batch\n",
        "                    numbers = numbers.to(device)\n",
        "                    operator = operator.to(device)\n",
        "                    predictions = self.model(numbers, operator)\n",
        "                    base_abstained = torch.isclose(\n",
        "                        predictions.squeeze(),\n",
        "                        torch.tensor(-1.0).to(device),\n",
        "                        rtol=0.1, atol=0.1\n",
        "                    )\n",
        "                    # Binary search for nearest boundary\n",
        "                    max_dist = 0.5\n",
        "                    min_dist = 0.001\n",
        "                    left = 0\n",
        "                    right = max_dist\n",
        "                    while right - left > min_dist:\n",
        "                        mid = (left + right) / 2\n",
        "                        search_dir = self.get_random_direction()\n",
        "                        # Test perturbed behavior\n",
        "                        for name, param in self.model.named_parameters():\n",
        "                            perturbed = new_param + mid * search_dir[name]\n",
        "                            param.data.copy_(perturbed)\n",
        "                        perturbed_pred = self.model(numbers, operator)\n",
        "                        perturbed_abstained = torch.isclose(\n",
        "                            perturbed_pred.squeeze(),\n",
        "                            torch.tensor(-1.0).to(device),\n",
        "                            rtol=0.1, atol=0.1\n",
        "                        )\n",
        "                        # Update search interval\n",
        "                        if (perturbed_abstained != base_abstained).any():\n",
        "                            right = mid\n",
        "                        else:\n",
        "                            left = mid\n",
        "                    distance_surface[i,j] = left\n",
        "            if i % 10 == 0 and j == 0:\n",
        "                 print(f\"Computing DDB row {i}/{self.grid_size}\")\n",
        "        # Restore original parameters\n",
        "        for name, param in self.model.named_parameters():\n",
        "            param.data.copy_(original_params[name])\n",
        "        return alphas, betas, distance_surface\n",
        "def get_rand_dirs(model: torch.nn.Module) -> Dict[str, Tensor]:\n",
        "    \"\"\"Generate a random direction with filter-wise normalizatio\n",
        "    Creates random directions that preserve the scaling of weights in\n",
        "    each filter of convolutional layer\n",
        "    Args:\n",
        "        model: Neural network mod\n",
        "    Returns:\n",
        "        Dict mapping parameter names to normalized random direction tensors\n",
        "    \"\"\"\n",
        "    direction = {}\n",
        "    # Sort parameters for consistency\n",
        "    params = sorted(model.named_parameters(), key=lambda x: x[0])\n",
        "    with torch.random.fork_rng():\n",
        "        torch.manual_seed(seed)\n",
        "        for name, param in params:\n",
        "            rnd = torch.randn_like(param)\n",
        "            # Normalize each filter independently\n",
        "            if len(param.size()) > 1:\n",
        "                for dim in range(rnd.size(0)):\n",
        "                    filter_norm = torch.norm(param[dim].data)\n",
        "                    dir_norm = torch.norm(rnd[dim].data) + 1e-8\n",
        "                    rnd[dim].data.mul_(filter_norm / dir_norm)\n",
        "            direction[name] = rnd\n",
        "    return direction\n",
        "def plot_high_quality_loss_landscape(model, data_loader, save_path,\n",
        "                             grid_resolution=200, alpha_range=1.0,\n",
        "                             azimuth=140, elevation=20,\n",
        "                             show_axes=False,  # Default False\n",
        "                             transparent_background=True,\n",
        "                             loss_computer=None,\n",
        "                             dark_mode=False):  # Default light theme\n",
        "    \"\"\"\n",
        "    Create loss landscape visualization.\n",
        "\n",
        "    Args:\n",
        "        azimuth (float): Horizontal rotation (0-360 degrees)\n",
        "        elevation (float): Vertical rotation (-90 to 90 degrees)\n",
        "        show_axes (bool): Whether to show axes and grid\n",
        "        transparent_background (bool): Use transparent background for paper\n",
        "        dark_mode (bool): Use dark theme (default False for paper)\n",
        "    \"\"\"\n",
        "    orig_params = {name: param.data.clone() for name, param in model.named_parameters()}\n",
        "    if loss_computer is None:\n",
        "      loss_computer = UnifiedLossComputer()\n",
        "\n",
        "\n",
        "    # Temporarily increment seed for second direction\n",
        "    global seed\n",
        "    dir1 = get_rand_dirs(model)  # Uses original seed\n",
        "    seed += 1  # Explicit increment\n",
        "    dir2 = get_rand_dirs(model)  # Uses incremented seed\n",
        "    seed -= 1  # Reset to original\n",
        "\n",
        "    # Single-pass with moderate resolution\n",
        "    alphas = np.linspace(-alpha_range, alpha_range, grid_resolution)\n",
        "    betas = np.linspace(-alpha_range, alpha_range, grid_resolution)\n",
        "    alpha_grid, beta_grid = np.meshgrid(alphas, betas)\n",
        "    loss_surface = np.zeros((grid_resolution, grid_resolution))\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\nComputing loss surface ({grid_resolution} x {grid_resolution})...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(grid_resolution), desc=\"Computing landscape\"):\n",
        "            for j in range(grid_resolution):\n",
        "                alpha, beta = alphas[i], betas[j]\n",
        "\n",
        "                # Update model parameters\n",
        "                for name, param in model.named_parameters():\n",
        "                    new_param = orig_params[name] + alpha * dir1[name] + beta * dir2[name]\n",
        "                    param.data.copy_(new_param)\n",
        "\n",
        "                total_loss = 0.0\n",
        "                batch_count = 0\n",
        "                max_batches = 7  # Reduced from 10\n",
        "\n",
        "                for batch in data_loader:\n",
        "                    numbers, operator, targets = [t.to(device) for t in batch]\n",
        "                    predictions = model(numbers, operator)\n",
        "                    loss, _ = loss_computer.compute_loss(predictions, targets, numbers, operator)\n",
        "                    total_loss += loss.item()\n",
        "                    batch_count += 1\n",
        "                    if batch_count >= max_batches:\n",
        "                        break\n",
        "\n",
        "                loss_surface[i, j] = total_loss / batch_count\n",
        "\n",
        "    # Restore original parameters\n",
        "    for name, param in model.named_parameters():\n",
        "        param.data.copy_(orig_params[name])\n",
        "\n",
        "    plt.style.use('default' if not dark_mode else 'dark_background')\n",
        "    fig = plt.figure(figsize=(16, 16), dpi=300)  # Restored original high quality size\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "    if dark_mode:\n",
        "        colors_map = [\n",
        "            (0.1, 0.0, 0.0),      # Dark base\n",
        "            (0.3, 0.0, 0.0),      # Dark red\n",
        "            (0.5, 0.1, 0.1),      # Medium red\n",
        "            (0.7, 0.2, 0.2),      # Light red\n",
        "            (1.0, 0.4, 0.4)       # Highlight\n",
        "        ]\n",
        "    else:\n",
        "        colors_map = [\n",
        "            (1.0, 0.8, 0.8),      # Light pink\n",
        "            (0.9, 0.6, 0.6),      # Salmon\n",
        "            (0.8, 0.4, 0.4),      # Medium red\n",
        "            (0.7, 0.2, 0.2),      # Deep red\n",
        "            (0.6, 0.0, 0.0)       # Dark red\n",
        "        ]\n",
        "\n",
        "    color_map = colors.LinearSegmentedColormap.from_list('paper_map', colors_map, N=256)\n",
        "\n",
        "    # Surface processing with slightly increased smoothing\n",
        "    loss_surface_processed = loss_surface.copy()\n",
        "    sigma = grid_resolution/35  # Slightly increased smoothing\n",
        "    loss_surface_processed = gaussian_filter(loss_surface_processed, sigma=sigma)\n",
        "\n",
        "    loss_surface_processed = np.log1p(loss_surface_processed)\n",
        "    p_min, p_max = np.percentile(loss_surface_processed, [1, 99])\n",
        "    loss_surface_processed = np.clip(loss_surface_processed, p_min, p_max)\n",
        "    loss_surface_processed = (loss_surface_processed - p_min) / (p_max - p_min)\n",
        "\n",
        "    # Enhanced lighting for paper clarity\n",
        "    ls = colors.LightSource(azdeg=315, altdeg=45)\n",
        "    illuminated_surface = ls.shade(loss_surface_processed,\n",
        "                                 cmap=color_map,\n",
        "                                 vert_exag=2.0,\n",
        "                                 blend_mode='soft')\n",
        "\n",
        "    # Main surface plot \n",
        "    surf = ax.plot_surface(alpha_grid, beta_grid, loss_surface_processed,\n",
        "                          facecolors=illuminated_surface,\n",
        "                          linewidth=0.1,\n",
        "                          antialiased=True,\n",
        "                          shade=True)\n",
        "\n",
        "    if show_axes:\n",
        "        # Contours only if axes are shown\n",
        "        levels = np.linspace(loss_surface_processed.min(), loss_surface_processed.max(), 20)\n",
        "        ax.contour(alpha_grid, beta_grid, loss_surface_processed,\n",
        "                  zdir='z',\n",
        "                  offset=loss_surface_processed.min(),\n",
        "                  levels=levels,\n",
        "                  cmap=color_map,\n",
        "                  alpha=0.3,\n",
        "                  linewidths=0.5)\n",
        "\n",
        "        # Configure axes\n",
        "        ax.grid(True, alpha=0.2, linestyle='-')\n",
        "        ax.set_xlabel('α', labelpad=10)\n",
        "        ax.set_ylabel('β', labelpad=10)\n",
        "        ax.set_zlabel('Loss', labelpad=10)\n",
        "    else:\n",
        "        # Hide all axes, ticks, and grid\n",
        "        ax.set_axis_off()\n",
        "\n",
        "    # Set view angle - rotated to face camera\n",
        "    ax.view_init(elev=elevation, azim=azimuth)\n",
        "    ax.dist = 8\n",
        "\n",
        "    # Configure background\n",
        "    if transparent_background:\n",
        "        ax.set_facecolor('none')\n",
        "        fig.patch.set_alpha(0.0)\n",
        "    else:\n",
        "        ax.set_facecolor('white' if not dark_mode else 'black')\n",
        "        fig.patch.set_facecolor('white' if not dark_mode else 'black')\n",
        "\n",
        "    # Hide panes when axes are off\n",
        "    if not show_axes:\n",
        "        ax.xaxis.pane.fill = False\n",
        "        ax.yaxis.pane.fill = False\n",
        "        ax.zaxis.pane.fill = False\n",
        "        ax.xaxis.pane.set_edgecolor('none')\n",
        "        ax.yaxis.pane.set_edgecolor('none')\n",
        "        ax.zaxis.pane.set_edgecolor('none')\n",
        "\n",
        "    # Set axis limits with minimal margin\n",
        "    margin = 0.05\n",
        "    ax.set_xlim(-alpha_range * (1 + margin), alpha_range * (1 + margin))\n",
        "    ax.set_ylim(-alpha_range * (1 + margin), alpha_range * (1 + margin))\n",
        "\n",
        "    \n",
        "    plt.savefig(save_path,\n",
        "                dpi=300,\n",
        "                bbox_inches='tight',\n",
        "                pad_inches=0.1,  # Reduced padding for paper\n",
        "                facecolor='none' if transparent_background else ('white' if not dark_mode else 'black'),\n",
        "                edgecolor='none',\n",
        "                transparent=transparent_background)\n",
        "    plt.close()\n",
        "\n",
        "    return loss_surface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiTbIQDhHeAx"
      },
      "outputs": [],
      "source": [
        "class UnifiedTracker:\n",
        "    \"\"\"\n",
        "    Unified system for tracking both computational resources and training metrics.\n",
        "\n",
        "    This tracker combines computational overhead tracking (FLOPs, passes, time)\n",
        "    with detailed training metrics (loss, accuracy, abstentions). It provides\n",
        "    a single interface for comprehensive monitoring of model training.\n",
        "\n",
        "    Attributes:\n",
        "        metrics (DefaultDict): Stores basic metrics by phase and epoch\n",
        "        current_epoch (int): Current training epoch\n",
        "        batch_abstention_counts (DefaultDict): Tracks abstention statistics\n",
        "        batch_totals (DefaultDict): Tracks total abstentions\n",
        "        forward_passes (int): Number of forward passes\n",
        "        backward_passes (int): Number of backward passes\n",
        "        flops (int): Total floating point operations\n",
        "        start_time (float): Training start timestamp\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        # Metrics tracking initialization\n",
        "        self.metrics: DefaultDict[str, DefaultDict[int, List[float]]] = defaultdict(\n",
        "            lambda: defaultdict(list)\n",
        "        )\n",
        "        self.current_epoch: int = 0\n",
        "\n",
        "        # Abstention tracking\n",
        "        self.batch_abstention_counts: DefaultDict[str, DefaultDict[int, Dict[str, int]]] = defaultdict(\n",
        "            lambda: defaultdict(\n",
        "                lambda: {\n",
        "                    'correct': 0,    # Abstained on invalid samples\n",
        "                    'incorrect': 0   # Abstained on valid samples\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "        self.batch_totals: DefaultDict[str, DefaultDict[int, int]] = defaultdict(\n",
        "            lambda: defaultdict(int)\n",
        "        )\n",
        "\n",
        "        # Computational tracking initialization\n",
        "        self.forward_passes: int = 0\n",
        "        self.backward_passes: int = 0\n",
        "        self.flops: int = 0\n",
        "        self.start_time: float = time.time()\n",
        "\n",
        "    def update_computational_metrics(\n",
        "        self,\n",
        "        forward_passes: int = 0,\n",
        "        backward_passes: int = 0,\n",
        "        flops: int = 0\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Update computational resource counters.\n",
        "\n",
        "        Args:\n",
        "            forward_passes: Number of forward passes to add\n",
        "            backward_passes: Number of backward passes to add\n",
        "            flops: Number of floating point operations to add\n",
        "        \"\"\"\n",
        "        self.forward_passes += forward_passes\n",
        "        self.backward_passes += backward_passes\n",
        "        self.flops += flops\n",
        "\n",
        "    def update_training_metrics(\n",
        "        self,\n",
        "        batch_metrics: Dict[str, Union[float, int]],\n",
        "        phase: str = 'train'\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Update training metrics from a batch.\n",
        "\n",
        "        Args:\n",
        "            batch_metrics: Dictionary containing:\n",
        "                - 'loss': batch loss value\n",
        "                - 'total_accuracy': batch accuracy\n",
        "                - 'total_abstentions': total abstentions\n",
        "                - 'abstained_on_invalid': correct abstentions\n",
        "                - 'abstained_on_valid': incorrect abstentions\n",
        "            phase: Training phase ('train' or 'val')\n",
        "        \"\"\"\n",
        "        # Update basic metrics\n",
        "        self.metrics[f\"{phase}_loss\"][self.current_epoch].append(batch_metrics['loss'])\n",
        "        self.metrics[f\"{phase}_accuracy\"][self.current_epoch].append(\n",
        "            batch_metrics['total_accuracy']\n",
        "        )\n",
        "\n",
        "        # Update abstention statistics\n",
        "        if batch_metrics['total_abstentions'] > 0:\n",
        "            self.batch_abstention_counts[phase][self.current_epoch]['correct'] += \\\n",
        "                batch_metrics['abstained_on_invalid']\n",
        "            self.batch_abstention_counts[phase][self.current_epoch]['incorrect'] += \\\n",
        "                batch_metrics['abstained_on_valid']\n",
        "            self.batch_totals[phase][self.current_epoch] += batch_metrics['total_abstentions']\n",
        "\n",
        "    def get_computational_metrics(self) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Get current computational resource usage metrics.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing:\n",
        "                - forward_passes: Total forward passes\n",
        "                - backward_passes: Total backward passes\n",
        "                - total_flops: Cumulative FLOPs\n",
        "                - wall_time: Total elapsed time\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'forward_passes': self.forward_passes,\n",
        "            'backward_passes': self.backward_passes,\n",
        "            'total_flops': self.flops,\n",
        "            'wall_time': time.time() - self.start_time\n",
        "        }\n",
        "\n",
        "    def get_training_metrics(self, phase: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Get aggregated training metrics for current epoch.\n",
        "\n",
        "        Args:\n",
        "            phase: Training phase to get metrics for\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing:\n",
        "                - loss: Mean epoch loss\n",
        "                - accuracy: Mean epoch accuracy\n",
        "                - correct_abstentions_percent: Correct abstention percentage\n",
        "                - incorrect_abstentions_percent: Incorrect abstention percentage\n",
        "        \"\"\"\n",
        "        epoch_data: Dict[str, float] = {}\n",
        "\n",
        "        # Calculate mean metrics\n",
        "        for key, epoch_dict in self.metrics.items():\n",
        "            if key.startswith(phase):\n",
        "                metric_name = key.split('_', 1)[1]\n",
        "                values = epoch_dict[self.current_epoch]\n",
        "                epoch_data[metric_name] = float(np.mean(values)) if values else 0.0\n",
        "\n",
        "        # Calculate abstention percentages\n",
        "        total_abstentions = self.batch_totals[phase][self.current_epoch]\n",
        "        if total_abstentions > 0:\n",
        "            correct_abs = self.batch_abstention_counts[phase][self.current_epoch]['correct']\n",
        "            incorrect_abs = self.batch_abstention_counts[phase][self.current_epoch]['incorrect']\n",
        "            epoch_data['correct_abstentions_percent'] = (correct_abs / total_abstentions) * 100.0\n",
        "            epoch_data['incorrect_abstentions_percent'] = (incorrect_abs / total_abstentions) * 100.0\n",
        "        else:\n",
        "            epoch_data['correct_abstentions_percent'] = 0.0\n",
        "            epoch_data['incorrect_abstentions_percent'] = 0.0\n",
        "\n",
        "        return epoch_data\n",
        "\n",
        "    def get_all_metrics(self, phase: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Get comprehensive metrics combining both computational and training statistics.\n",
        "\n",
        "        Args:\n",
        "            phase: Training phase to get metrics for\n",
        "\n",
        "        Returns:\n",
        "            Combined dictionary of all metrics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            **self.get_computational_metrics(),\n",
        "            **self.get_training_metrics(phase)\n",
        "        }\n",
        "\n",
        "    def count_flops(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        input_batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "    ) -> int:\n",
        "        \"\"\"\n",
        "        Calculate total FLOPs for a forward pass through the ArithmeticNet architecture.\n",
        "\n",
        "        This function provides a layer-by-layer accounting of floating point operations\n",
        "        in an ArithmeticNet model. It counts operations for:\n",
        "        1. Input processing (number embedding and operator embedding)\n",
        "        2. Core network layers (three fully connected layers)\n",
        "        3. Output layer\n",
        "\n",
        "        Mathematical Details:\n",
        "        -------------------\n",
        "        For each linear layer:\n",
        "            FLOPs = batch_size * in_features * out_features\n",
        "\n",
        "        For embeddings:\n",
        "            FLOPs = batch_size * embedding_dim\n",
        "\n",
        "        Args:\n",
        "            model: ArithmeticNet model instance\n",
        "            input_batch: Tuple of (numbers, operator, targets) tensors\n",
        "\n",
        "        Returns:\n",
        "            Total number of floating point operations for a forward pass\n",
        "\n",
        "        Note:\n",
        "            This is a simplified FLOP count that focuses on major operations.\n",
        "            It does not count activation functions, bias additions, or other\n",
        "            minor operations.\n",
        "        \"\"\"\n",
        "        numbers, operator, _ = input_batch\n",
        "        batch_size = len(numbers)\n",
        "        total_flops = 0\n",
        "\n",
        "        # Input processing FLOPs\n",
        "        # Number embedding: two numbers per sample\n",
        "        total_flops += batch_size * 2 * model.num_embedder.out_features\n",
        "        # Operator embedding\n",
        "        total_flops += batch_size * model.op_embedding.embedding_dim\n",
        "\n",
        "        # Core network FLOPs\n",
        "        # Each linear layer: matrix multiplication\n",
        "        total_flops += batch_size * model.layer1.in_features * model.layer1.out_features\n",
        "        total_flops += batch_size * model.layer2.in_features * model.layer2.out_features\n",
        "        total_flops += batch_size * model.layer3.in_features * model.layer3.out_features\n",
        "\n",
        "        # Output layer FLOPs\n",
        "        total_flops += batch_size * model.output.in_features * model.output.out_features\n",
        "\n",
        "        return total_flops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPrVtw_5IVyR"
      },
      "outputs": [],
      "source": [
        "class BaseTrainer: #REWRITE Comment\n",
        "    \"\"\"\n",
        "    Base trainer class implementing abstention-aware training for neural computation models.\n",
        "\n",
        "    This trainer handles both valid and invalid computations, implementing an abstention\n",
        "    mechanism where the model can learn to abstain from predictions on invalid inputs.\n",
        "    The trainer includes comprehensive metrics tracking and loss computation that rewards\n",
        "    correct abstentions while penalizing incorrect ones.\n",
        "\n",
        "    Attributes:\n",
        "        model (nn.Module): The neural network model being trained\n",
        "        optimizer (optim.Optimizer): The optimizer used for training\n",
        "        criterion (nn.Module): Loss function for comparing predictions with targets\n",
        "        scheduler (optim.lr_scheduler.ReduceLROnPlateau): Learning rate scheduler\n",
        "        tracker (ComputationalTracker): Tracks computational overhead metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the trainer with a model and optimizer.\n",
        "\n",
        "        Args:\n",
        "            model: Neural network model to train\n",
        "            optimizer: Optimizer for updating model parameters\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = nn.MSELoss(reduction='none')\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "        self.loss_computer = UnifiedLossComputer(\n",
        "            abstention_weight=ABSTENTION_WEIGHT,\n",
        "        )\n",
        "        self.tracker = UnifiedTracker()\n",
        "\n",
        "    def training_step(\n",
        "        self,\n",
        "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Perform a single training step.\n",
        "\n",
        "        Args:\n",
        "            batch: Tuple of (numbers, operator, targets)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing batch metrics\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        # Track computational overhead\n",
        "        self.tracker.forward_passes += 1\n",
        "        self.tracker.backward_passes += 1\n",
        "        self.tracker.flops += self.tracker.count_flops(self.model, batch)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        numbers, operator, targets = batch\n",
        "        predictions = self.model(numbers, operator)\n",
        "        loss = self.loss_computer.compute_loss(predictions, targets, numbers, operator)[0]\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return self.compute_batch_metrics(numbers, operator, targets, predictions)\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        loader: DataLoader,\n",
        "        phase: str = 'val'\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate the model on a data loader.\n",
        "\n",
        "        Args:\n",
        "            loader: DataLoader containing evaluation data\n",
        "            phase: Evaluation phase ('train' or 'val')\n",
        "\n",
        "        Returns:\n",
        "            Average loss over the evaluation set\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                numbers, operator, targets = batch\n",
        "                predictions = self.model(numbers, operator)\n",
        "                batch_metrics = self.compute_batch_metrics(numbers, operator, targets, predictions)\n",
        "                self.metrics_tracker.update(batch_metrics, phase)\n",
        "                total_loss += batch_metrics['loss']\n",
        "\n",
        "        return total_loss / len(loader)\n",
        "\n",
        "    def compute_batch_metrics(\n",
        "        self,\n",
        "        numbers: Tensor,\n",
        "        operator: Tensor,\n",
        "        targets: Tensor,\n",
        "        predictions: Optional[Tensor] = None\n",
        "    ) -> Dict[str, float]:\n",
        "        if predictions is None:\n",
        "            predictions = self.model(numbers, operator)\n",
        "        return self.loss_computer.compute_metrics(predictions, targets, numbers, operator)\n",
        "class PGDTrainer(BaseTrainer):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) trainer that implements adversarial training\n",
        "    to enhance abstention robustness.\n",
        "\n",
        "    This trainer extends BaseTrainer by adding adversarial training specifically\n",
        "    for invalid computations. It uses PGD to find adversarial parameter perturbations\n",
        "    that would cause the model to output non-abstention values for invalid inputs,\n",
        "    then trains the model to resist these perturbations.\n",
        "\n",
        "    Mathematical Description:\n",
        "    -----------------------\n",
        "    For invalid inputs x, the PGD attack tries to find model parameters θ' that minimize:\n",
        "        L_adv(θ') = -||f_θ'(x) - y_fake||²\n",
        "\n",
        "    where f_θ' is the model with perturbed parameters and y_fake are random valid outputs.\n",
        "\n",
        "    Attributes:\n",
        "        alpha (float): PGD step size for gradient ascent\n",
        "        k (int): Number of PGD iterations\n",
        "        weight (float): Weight for adversarial loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        *,\n",
        "        alpha: float = 0.016,\n",
        "        k: int = 3,\n",
        "        weight: float = 6.5\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the PGD trainer.\n",
        "\n",
        "        Args:\n",
        "            model: Neural network model\n",
        "            optimizer: Optimizer for parameter updates\n",
        "            alpha: Step size for PGD\n",
        "            k: Number of PGD steps\n",
        "            weight: Weight for adversarial loss\n",
        "        \"\"\"\n",
        "        super().__init__(model, optimizer)\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "        self.weight = weight\n",
        "        self.tracker = UnifiedTracker()\n",
        "\n",
        "    def get_invalid_mask_for_pgd(\n",
        "        self,\n",
        "        invalid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get the mask for PGD training. Base implementation uses all invalid samples.\n",
        "\n",
        "        Args:\n",
        "            invalid_mask: Original invalid computation mask\n",
        "\n",
        "        Returns:\n",
        "            Mask indicating which samples to use for PGD\n",
        "        \"\"\"\n",
        "        return invalid_mask\n",
        "\n",
        "    def training_step(\n",
        "        self,\n",
        "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Perform a training step including both normal and adversarial updates.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        numbers, operator, targets = batch\n",
        "        invalid_mask = is_invalid_computation(numbers, operator)\n",
        "\n",
        "        # Track main forward pass\n",
        "        self.tracker.forward_passes += 1\n",
        "        self.tracker.flops += self.tracker.count_flops(self.model, batch)\n",
        "\n",
        "        # Standard training step\n",
        "        self.optimizer.zero_grad()\n",
        "        predictions = self.model(numbers, operator)\n",
        "        loss = self.loss_computer.compute_loss(predictions, targets, numbers, operator)[0]\n",
        "        loss.backward()\n",
        "\n",
        "        # Track backward pass\n",
        "        self.tracker.backward_passes += 1\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # PGD step for invalid computations\n",
        "        if invalid_mask.any():\n",
        "            pgd_mask = self.get_invalid_mask_for_pgd(invalid_mask)\n",
        "\n",
        "            # Track PGD computational cost\n",
        "            self.tracker.forward_passes += self.k\n",
        "            self.tracker.backward_passes += self.k\n",
        "            selected_batch = (\n",
        "                numbers[pgd_mask],\n",
        "                operator[pgd_mask],\n",
        "                targets[pgd_mask]\n",
        "            )\n",
        "            self.tracker.flops += self.k * self.tracker.count_flops(self.model, selected_batch)\n",
        "\n",
        "            self.perform_pgd(batch, pgd_mask)\n",
        "\n",
        "        return self.compute_batch_metrics(numbers, operator, targets, predictions)\n",
        "\n",
        "    def perform_pgd(\n",
        "        self,\n",
        "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "        invalid_mask: torch.Tensor\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Perform PGD-based adversarial training on invalid inputs.\n",
        "\n",
        "        1. Store original parameters θ₀\n",
        "        2. For k steps:\n",
        "           a. Forward pass: ŷ = f_θ(x_invalid)\n",
        "           b. Compute anti-abstention loss: L = -||ŷ - y_fake||²\n",
        "           c. Update: θ ← θ + α * ∇_θ L\n",
        "        3. Restore θ₀\n",
        "        4. Final update with combined loss\n",
        "\n",
        "        Args:\n",
        "            batch: Tuple of (numbers, operator, targets)\n",
        "            invalid_mask: Boolean mask identifying invalid computations\n",
        "        \"\"\"\n",
        "        # Store original parameters\n",
        "        original_params = {\n",
        "            name: param.clone().detach()\n",
        "            for name, param in self.model.named_parameters()\n",
        "        }\n",
        "\n",
        "        # Extract invalid inputs\n",
        "        numbers, operator, targets = batch\n",
        "        invalid_numbers = numbers[invalid_mask]\n",
        "        invalid_operator = operator[invalid_mask]\n",
        "\n",
        "        # Generate anti-abstention targets\n",
        "        # boundary targets\n",
        "        num_invalid = invalid_mask.sum()\n",
        "        fake_targets = self.generate_pgd_targets(invalid_numbers, invalid_operator)\n",
        "\n",
        "        # PGD iteration loop\n",
        "        for _ in range(self.k):\n",
        "            self.optimizer.zero_grad()\n",
        "            predictions = self.model(invalid_numbers, invalid_operator)\n",
        "\n",
        "            # Compute anti-abstention loss\n",
        "            # Negative MSE pushes predictions away from abstention token\n",
        "            anti_abstention_loss = -F.mse_loss(predictions, fake_targets)\n",
        "            (self.weight * anti_abstention_loss).backward()\n",
        "\n",
        "            # Gradient ascent step\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param.data += self.alpha * param.grad\n",
        "\n",
        "        # Restore original parameters\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.model.named_parameters():\n",
        "                param.data.copy_(original_params[name])\n",
        "\n",
        "        # Final update with standard loss\n",
        "        self.optimizer.zero_grad()\n",
        "        predictions = self.model(numbers, operator)\n",
        "        loss = self.loss_computer.compute_loss(predictions, targets, numbers, operator)[0]\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    def generate_pgd_targets(self, invalid_numbers, invalid_operator):\n",
        "        targets = []\n",
        "        for num, op in zip(invalid_numbers, invalid_operator):\n",
        "            if op == '+':\n",
        "                targets.append(395 + torch.rand(1) * 5)\n",
        "            elif op == '-':\n",
        "                targets.append(torch.rand(1) * 5)\n",
        "            else:\n",
        "                boundary_points = torch.tensor([0, 400])\n",
        "                chosen_boundary = boundary_points[torch.randint(0, 2, (1,))]\n",
        "                targets.append(chosen_boundary + (torch.rand(1) - 0.5) * 20)\n",
        "        return torch.tensor(targets, device=invalid_numbers.device).unsqueeze(1)  # Add dimension to match expected shape\n",
        "class EfficientPGDTrainer(PGDTrainer):\n",
        "    \"\"\"\n",
        "    Memory-efficient version of PGD trainer that performs adversarial training\n",
        "    on a randomly selected subset of invalid computations.\n",
        "\n",
        "    This implementation reduces memory usage and computation time by applying\n",
        "    PGD to only a fraction of invalid samples in each batch, while maintaining\n",
        "    the effectiveness of adversarial training.\n",
        "\n",
        "    Attributes:\n",
        "        sample_ratio (float): Fraction of invalid samples to use for PGD\n",
        "        seed (int): Random seed for sample selection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        *,  # Force keyword arguments for clarity\n",
        "        alpha: float = 0.016,\n",
        "        k: int = 2,\n",
        "        weight: float = 6.5,\n",
        "        sample_ratio: float = 0.3,\n",
        "        seed: int = 42\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the efficient PGD trainer.\n",
        "\n",
        "        Args:\n",
        "            model: Neural network model\n",
        "            optimizer: Optimizer for parameter updates\n",
        "            alpha: Step size for PGD\n",
        "            k: Number of PGD steps\n",
        "            weight: Weight for adversarial loss\n",
        "            sample_ratio: Fraction of invalid samples to use for PGD\n",
        "            seed: Random seed for sample selection\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            model,\n",
        "            optimizer,\n",
        "            alpha=alpha,\n",
        "            k=k,\n",
        "            weight=weight\n",
        "        )\n",
        "        self.sample_ratio = sample_ratio\n",
        "        self.seed = seed\n",
        "\n",
        "    def get_invalid_mask_for_pgd(\n",
        "        self,\n",
        "        invalid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Select a random subset of invalid samples for PGD training.\n",
        "\n",
        "        Args:\n",
        "            invalid_mask: Original invalid computation mask\n",
        "\n",
        "        Returns:\n",
        "            Reduced mask for subset of samples to use in PGD\n",
        "        \"\"\"\n",
        "        mask_indices = torch.where(invalid_mask)[0]\n",
        "        num_samples = max(1, int(len(mask_indices) * self.sample_ratio))\n",
        "\n",
        "        with torch.random.fork_rng():\n",
        "            torch.manual_seed(self.seed)\n",
        "            selected_indices = mask_indices[\n",
        "                torch.randperm(len(mask_indices))[:num_samples]\n",
        "            ]\n",
        "\n",
        "        reduced_mask = torch.zeros_like(invalid_mask)\n",
        "        reduced_mask[selected_indices] = True\n",
        "\n",
        "        return reduced_mask\n",
        "class InputSpaceAdversarialTrainer(BaseTrainer):\n",
        "    \"\"\"\n",
        "    A control trainer that does input-space adversarial training\n",
        "    without increasing the effective dataset size and without\n",
        "    leaking test data. This ensures it is a fair control\n",
        "    relative to your parameter-based PGD model.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        *,\n",
        "        epsilon: float = 0.1,\n",
        "        steps: int = 3,\n",
        "        adv_ratio: float = 0.05,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: Neural network model\n",
        "            optimizer: Optimizer for parameter updates\n",
        "            epsilon: Max perturbation magnitude for valid inputs\n",
        "            steps: Number of small PGD steps\n",
        "            adv_ratio: Fraction of training examples in each batch to perturb\n",
        "        \"\"\"\n",
        "        super().__init__(model, optimizer)\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.adv_ratio = adv_ratio\n",
        "        self.tracker = UnifiedTracker()\n",
        "\n",
        "    def generate_input_adversarial_examples(\n",
        "        self,\n",
        "        numbers: torch.Tensor,\n",
        "        operator: torch.Tensor,\n",
        "        targets: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Generate adversarial examples *only* from the current batch.\n",
        "        We do not expand or leak data from test sets. We also keep the\n",
        "        final 'adv batch' size the same as the original subset we replace.\n",
        "        \"\"\"\n",
        "        device = next(self.model.parameters()).device\n",
        "        perturbed = numbers.clone().to(device).requires_grad_(True)\n",
        "        operator = operator.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # We'll only push the model's predictions away from the correct targets\n",
        "        # for these valid inputs. (No new data samples.)\n",
        "        for _ in range(self.steps):\n",
        "            predictions = self.model(perturbed, operator)\n",
        "\n",
        "            # Simple \"maximize MSE\" approach: push predictions away from targets\n",
        "            # so that the model is forced to learn a stable boundary.\n",
        "            loss = F.mse_loss(predictions, targets, reduction='mean')\n",
        "            loss = -loss  # negative for gradient ascent on the same model params\n",
        "\n",
        "            # Clear old grads & backprop\n",
        "            perturbed.grad = None\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if perturbed.grad is not None:\n",
        "                    # FGSM-like step: take sign of grad\n",
        "                    grad_sign = perturbed.grad.sign()\n",
        "                    update = self.epsilon * grad_sign\n",
        "                    perturbed = perturbed + update\n",
        "\n",
        "                    # If you clamp to [0,400], do so carefully so valid points can move\n",
        "                    # but do not get stuck at the boundary:\n",
        "                    perturbed = torch.clamp(perturbed, min=0, max=400)\n",
        "\n",
        "                # Re-enable grad for next iteration\n",
        "                perturbed = perturbed.detach().requires_grad_(True)\n",
        "\n",
        "        return perturbed.detach(), operator, targets\n",
        "\n",
        "    def training_step(\n",
        "        self,\n",
        "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Single training step:\n",
        "          1) Split the batch so that a fraction 'self.adv_ratio' is used for adversarial input\n",
        "          2) Generate adversarial perturbations for that fraction\n",
        "          3) Merge them back and run a forward/backward pass with the same total batch size\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        numbers, operator, targets = batch\n",
        "        batch_size = len(numbers)\n",
        "\n",
        "        # Track initial forward pass\n",
        "        self.tracker.forward_passes += 1\n",
        "        self.tracker.flops += self.tracker.count_flops(self.model, batch)\n",
        "\n",
        "        # Index at which to split\n",
        "        split_idx = int(batch_size * (1 - self.adv_ratio))\n",
        "\n",
        "        # 1) Clean portion\n",
        "        clean_nums = numbers[:split_idx]\n",
        "        clean_ops = operator[:split_idx]\n",
        "        clean_targets = targets[:split_idx]\n",
        "\n",
        "        # 2) Adversarial portion\n",
        "        # Track adversarial generation passes\n",
        "        adv_nums, adv_ops, adv_targets = self.generate_input_adversarial_examples(\n",
        "            numbers[split_idx:], operator[split_idx:], targets[split_idx:]\n",
        "        )\n",
        "        # Each adversarial example generation requires self.steps forward passes\n",
        "        self.tracker.forward_passes += self.steps * (batch_size - split_idx)\n",
        "        self.tracker.flops += self.steps * (batch_size - split_idx) * \\\n",
        "                             self.tracker.count_flops(self.model, (numbers[split_idx:],\n",
        "                                                                 operator[split_idx:],\n",
        "                                                                 targets[split_idx:]))\n",
        "\n",
        "        # Recombine so total size is the same as the original batch\n",
        "        combined_nums = torch.cat([clean_nums, adv_nums], dim=0)\n",
        "        combined_ops = torch.cat([clean_ops, adv_ops], dim=0)\n",
        "        combined_tgts = torch.cat([clean_targets, adv_targets], dim=0)\n",
        "\n",
        "        # Forward pass on the combined batch\n",
        "        self.optimizer.zero_grad()\n",
        "        predictions = self.model(combined_nums, combined_ops)\n",
        "        loss, _ = self.loss_computer.compute_loss(\n",
        "            predictions, combined_tgts, combined_nums, combined_ops\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        # Track final backward pass\n",
        "        self.tracker.backward_passes += 1\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        with torch.no_grad():\n",
        "            metrics = self.compute_batch_metrics(\n",
        "                combined_nums, combined_ops, combined_tgts, predictions\n",
        "            )\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP-qmMtZWF4M"
      },
      "outputs": [],
      "source": [
        "def get_model_name(model_idx: int, model: nn.Module, is_pgd: bool, model_var_name: Optional[str]) -> str:\n",
        "    \"\"\"\n",
        "    Convert variable name to formatted model name, with fallback to type-based naming.\n",
        "\n",
        "    Args:\n",
        "        model_idx: Index of the model in the training framework\n",
        "        model: The neural network model\n",
        "        is_pgd: Flag indicating if the model uses PGD training\n",
        "        model_var_name: Optional variable name provided for the model\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted model name for use in saving and tracking\n",
        "    \"\"\"\n",
        "    if model_var_name:\n",
        "        base_name = model_var_name.replace('_model', '').replace('_', ' ').strip()\n",
        "        return f\"model_{model_idx}_{base_name}\"\n",
        "    elif is_pgd:\n",
        "        return f\"model_{model_idx}_pgd\"\n",
        "    else:\n",
        "        return f\"model_{model_idx}_base_adam\"\n",
        "\n",
        "class UnifiedTrainingFramework:\n",
        "    def __init__(\n",
        "        self,\n",
        "        models: List[nn.Module],\n",
        "        trainers: List[BaseTrainer],\n",
        "        save_dir: str = 'training_results',\n",
        "        fixed_scale: int = 1000,\n",
        "        training_config: Optional[TrainingConfig] = None,\n",
        "        model_names: Optional[List[str]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the unified training framework for multiple models.\n",
        "\n",
        "        Args:\n",
        "            models: List of PyTorch models to train\n",
        "            save_dir: Directory to save training results and visualizations\n",
        "            lr: Learning rate for optimization\n",
        "            fixed_scale: Scale factor for landscape analysis\n",
        "            -1.0: Token value used for model abstention\n",
        "            training_config: Configuration object for training parameters\n",
        "            model_var_names: Optional list of variable names for models\n",
        "        \"\"\"\n",
        "        self.models = models\n",
        "        self.save_dir = save_dir\n",
        "        self.fixed_scale = fixed_scale\n",
        "        self.training_config = training_config or TrainingConfig()\n",
        "        self.trainers = []\n",
        "        self.trainers = trainers\n",
        "        self.model_names = model_names\n",
        "\n",
        "        if len(models) != len(trainers):\n",
        "          raise ValueError(\"Number of models must match number of trainers\")\n",
        "\n",
        "        # Create necessary directories\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(save_dir, 'landscapes'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(save_dir, 'metrics'), exist_ok=True)\n",
        "        # Initialize metrics history tracking\n",
        "        self.metrics_history = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    def get_model_name(model_idx: int, model: nn.Module, model_var_name: Optional[str]) -> str:\n",
        "        if model_var_name:\n",
        "            base_name = model_var_name.replace('_model', '').replace('_', ' ').strip()\n",
        "            return f\"model_{model_idx}_{base_name}\"\n",
        "        return f\"model_{model_idx}\"\n",
        "\n",
        "    def train_single_model(\n",
        "        self,\n",
        "        model_idx: int,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        epochs: int,\n",
        "        analyze_every: int\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Train a single model with the specified configuration.\n",
        "\n",
        "        Args:\n",
        "            model_idx: Index of the model to train\n",
        "            train_loader: DataLoader for training data\n",
        "            val_loader: DataLoader for validation data\n",
        "            epochs: Number of training epochs\n",
        "            analyze_every: Frequency of landscape analysis\n",
        "\n",
        "        Returns:\n",
        "            Dict containing training metrics history\n",
        "        \"\"\"\n",
        "\n",
        "        model = self.models[model_idx]\n",
        "        trainer = self.trainers[model_idx]\n",
        "        model_name = self.model_names[model_idx]\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            self._train_epoch(trainer, train_loader, epoch, total_epochs=epochs)\n",
        "\n",
        "            # Validation phase\n",
        "            if epoch % self.training_config.val_freq == 0:\n",
        "                self._validate_model(trainer, val_loader, epoch)\n",
        "\n",
        "            # Landscape analysis\n",
        "            if epoch > 0 and (epoch + 1) % self.training_config.analysis_freq == 0:\n",
        "                self.perform_analysis_single_model(trainer, epoch, val_loader,\n",
        "                                                model_name, model_idx)\n",
        "\n",
        "        # Save final results\n",
        "        #self._save_model_results(model_idx, model_name)\n",
        "       # self._log_computational_metrics(trainer, model_name)\n",
        "        return dict(self.metrics_history)\n",
        "    def _train_epoch(\n",
        "        self,\n",
        "        trainer: BaseTrainer,\n",
        "        train_loader: DataLoader,\n",
        "        epoch: int,\n",
        "        total_epochs: int,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Train for one epoch using the provided trainer.\n",
        "\n",
        "        Args:\n",
        "            trainer: Trainer instance to use\n",
        "            train_loader: DataLoader for training data\n",
        "            epoch: Current epoch number\n",
        "        \"\"\"\n",
        "        # Update training configuration epoch\n",
        "\n",
        "        # Initialize epoch metrics\n",
        "        epoch_metrics = defaultdict(list)\n",
        "\n",
        "        # Training loop\n",
        "        pbar = tqdm(train_loader,\n",
        "                    desc=f'Epoch {epoch+1}/{total_epochs}',\n",
        "                    leave=True)\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            # Perform training step\n",
        "            batch_metrics = trainer.training_step(batch)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            for key, value in batch_metrics.items():\n",
        "                epoch_metrics[f'train_{key}'].append(value)\n",
        "\n",
        "            # Update progress bar with running averages\n",
        "            running_metrics = {\n",
        "                key: np.mean(values[-100:])  # Last 100 batches\n",
        "                for key, values in epoch_metrics.items()\n",
        "            }\n",
        "            metric_strings = [\n",
        "              f\"\\n{key.replace('train_', '')}: {value:.4g}\"\n",
        "              for key, value in running_metrics.items()\n",
        "        ]\n",
        "\n",
        "            pbar.set_postfix(running_metrics)\n",
        "\n",
        "\n",
        "        # Calculate and store epoch averages\n",
        "        avg_metrics = {\n",
        "            key: float(np.mean(values))\n",
        "            for key, values in epoch_metrics.items()\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def _validate_model(\n",
        "        self,\n",
        "        trainer: BaseTrainer,\n",
        "        val_loader: DataLoader,\n",
        "        epoch: int\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Validate the model using the provided trainer.\n",
        "\n",
        "        Args:\n",
        "            trainer: Trainer instance to use\n",
        "            val_loader: DataLoader for validation data\n",
        "            epoch: Current epoch number\n",
        "\n",
        "        Returns:\n",
        "            float: Average validation loss\n",
        "        \"\"\"\n",
        "        # Initialize validation metrics\n",
        "        val_metrics = defaultdict(list)\n",
        "\n",
        "        # Validation loop\n",
        "        trainer.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                # Get predictions and metrics\n",
        "                numbers, operator, targets = batch\n",
        "                predictions = trainer.model(numbers, operator)\n",
        "                batch_metrics = trainer.compute_batch_metrics(\n",
        "                    numbers=numbers,\n",
        "                    operator=operator,\n",
        "                    targets=targets,\n",
        "                    predictions=predictions\n",
        "                )\n",
        "\n",
        "                # Accumulate metrics\n",
        "                for key, value in batch_metrics.items():\n",
        "                    val_metrics[f'val_{key}'].append(value)\n",
        "\n",
        "        # Calculate validation averages\n",
        "        avg_metrics = {\n",
        "            key: float(np.mean(values))\n",
        "            for key, values in val_metrics.items()\n",
        "        }\n",
        "\n",
        "        # Update metrics history\n",
        "        for key, value in avg_metrics.items():\n",
        "            self.metrics_history[key][epoch].append(value)\n",
        "\n",
        "        # Log validation results\n",
        "        metrics_str = ', '.join(\n",
        "            f'{key}: {value:.4f}'\n",
        "            for key, value in avg_metrics.items()\n",
        "        )\n",
        "        print(f'\\nEpoch {epoch} validation: {metrics_str}')\n",
        "\n",
        "        # Step learning rate scheduler if it exists\n",
        "        val_loss = avg_metrics['val_loss']\n",
        "        if hasattr(trainer, 'scheduler'):\n",
        "            trainer.scheduler.step(val_loss)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def _log_computational_metrics(\n",
        "        self,\n",
        "        trainer: BaseTrainer,\n",
        "        model_name: str\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Log computational metrics from trainer.\n",
        "\n",
        "        Args:\n",
        "            trainer: Trainer instance\n",
        "            model_name: Name of the model\n",
        "        \"\"\"\n",
        "        if hasattr(trainer, 'tracker'):\n",
        "            metrics = trainer.tracker.get_metrics()\n",
        "\n",
        "            # Save detailed metrics to file\n",
        "            metrics_path = os.path.join(\n",
        "                self.save_dir,\n",
        "                'metrics',\n",
        "                f'{model_name}_computational_metrics.json'\n",
        "            )\n",
        "            with open(metrics_path, 'w') as f:\n",
        "                json.dump(metrics, f, indent=2)\n",
        "\n",
        "            # Print summary\n",
        "            print(f\"\\nComputational metrics for {model_name}:\")\n",
        "            print(f\"  Forward passes: {metrics['forward_passes']}\")\n",
        "            print(f\"  Backward passes: {metrics['backward_passes']}\")\n",
        "            print(f\"  Total FLOPs: {metrics['total_flops']:,}\")\n",
        "            print(f\"  Wall time: {metrics['wall_time']:.2f}s\")\n",
        "\n",
        "    def perform_analysis_single_model(\n",
        "        self,\n",
        "        trainer: Union[BaseTrainer, EfficientPGDTrainer],\n",
        "        epoch: int,\n",
        "        val_loader: DataLoader,\n",
        "        model_name: str,\n",
        "        model_idx: int\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Perform comprehensive landscape analysis on a single model using both\n",
        "        principal component and random direction analysis.\n",
        "\n",
        "        Args:\n",
        "            trainer: The model's trainer instance\n",
        "            epoch: Current training epoch\n",
        "            val_loader: Validation data loader\n",
        "            model_name: Name of the model\n",
        "            model_idx: Index of the model\n",
        "        \"\"\"\n",
        "        print(f\"\\nPerforming landscape analysis for epoch {epoch} on {model_name}...\")\n",
        "\n",
        "        # Setup directory structure\n",
        "        landscape_dir = os.path.join(self.save_dir, 'landscapes', model_name)\n",
        "        os.makedirs(landscape_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize analyzer\n",
        "        analyzer = LandscapeAnalyzer(\n",
        "            self.models[model_idx],\n",
        "            save_dir=landscape_dir,\n",
        "            fixed_scale=self.fixed_scale,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # First analyze using random directions\n",
        "            save_path = os.path.join(landscape_dir, f\"epoch_{epoch}_random_directions.png\")\n",
        "            analyzer.visualize_landscape(\n",
        "                val_loader,\n",
        "                epoch,\n",
        "                model_name=f\"{model_name}_random\",\n",
        "                rand_dir=True\n",
        "            )\n",
        "\n",
        "            # Then analyze using principal directions\n",
        "            save_path = os.path.join(landscape_dir, f\"epoch_{epoch}_principal_directions.png\")\n",
        "            analyzer.visualize_landscape(\n",
        "                val_loader,\n",
        "                epoch,\n",
        "                model_name=f\"{model_name}_pca\",\n",
        "                rand_dir=False\n",
        "            )\n",
        "\n",
        "            # Perform numerical landscape analysis\n",
        "            val_batch = next(iter(val_loader))\n",
        "            metrics = analyzer.analyze_landscape(val_batch, epoch)\n",
        "\n",
        "            # Log metrics\n",
        "            print(f\"  [Landscape] Alpha Sharpness: {metrics['alpha_sharpness']:.4f}\")\n",
        "            print(\"  [Landscape] Top Eigenvalues:\")\n",
        "            for i, ev in enumerate(metrics['top_eigenvalues']):\n",
        "                print(f\"    λ{i+1}: {ev:.4f}\")\n",
        "\n",
        "            print(\"  [Landscape] Multiscale Sharpness:\")\n",
        "            for scale, value in metrics['multiscale_sharpness'].items():\n",
        "                print(f\"    {scale}: {value:.4f}\")\n",
        "\n",
        "            print(f\"  [Landscape] Valley Asymmetry: {metrics['valley_asymmetry']:.4f}\")\n",
        "\n",
        "            # Store metrics history\n",
        "            for key, value in metrics.items():\n",
        "                self.metrics_history[model_name][f\"landscape_{key}\"].append((epoch, value))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to perform landscape analysis:\")\n",
        "            print(traceback.format_exc())\n",
        "            # Ensure directory exists even if analysis fails\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    def evaluate_and_visualize(\n",
        "        self,\n",
        "        test_loader: DataLoader,\n",
        "        epoch: int,\n",
        "        model_names: List[str]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Evaluate models on test data and generate visualizations.\n",
        "\n",
        "        Args:\n",
        "            test_loader: DataLoader for test data\n",
        "            epoch: Current epoch number\n",
        "            model_names: List of model names to evaluate\n",
        "        \"\"\"\n",
        "        abstention_rates = defaultdict(list)\n",
        "\n",
        "        # Evaluate each model\n",
        "        for model_idx, model in enumerate(self.models):\n",
        "            model_name = model_names[model_idx]\n",
        "            metrics = self._evaluate_single_model(model, test_loader, model_name)\n",
        "            abstention_rates[model_name].append(metrics['total_abstentions'])\n",
        "\n",
        "        # Generate visualization\n",
        "        self._plot_abstention_rates(abstention_rates, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsCP_dujM2nN"
      },
      "outputs": [],
      "source": [
        "def test_model_with_noise(\n",
        "    model: nn.Module,\n",
        "    loader: Iterator[tuple[Tensor, Tensor, Tensor]],\n",
        "    noise_std: float\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Test a model's performance with optional additive Gaussian noise.\n",
        "\n",
        "    Args:\n",
        "        model: Neural network model for arithmetic operations\n",
        "        loader: DataLoader providing batches of (numbers, operator, targets)\n",
        "        noise_std: Standard deviation of Gaussian noise to add to inputs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing various performance metrics:\n",
        "        - abstention_rate: Frequency of model abstaining from predictions\n",
        "        - invalid_recall: Recall for invalid computation cases\n",
        "        - valid_recall: Recall for valid computation cases\n",
        "        - correct_abstention: Rate of correctly abstaining on invalid cases\n",
        "        - valid_accuracy: Accuracy on valid cases when not abstaining\n",
        "    \"\"\"\n",
        "    metrics = defaultdict(list)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            numbers, operator, targets = batch\n",
        "            # Add Gaussian noise to input if specified\n",
        "            if noise_std > 0:\n",
        "                noise = torch.randn_like(numbers) * noise_std\n",
        "                numbers = numbers + noise\n",
        "\n",
        "            # Get model predictions and ensure consistent shapes\n",
        "            predictions = model(numbers, operator)\n",
        "            predictions = predictions.squeeze()\n",
        "            targets = targets.squeeze()\n",
        "\n",
        "            # Identify invalid computations and abstentions\n",
        "            invalid_mask = is_invalid_computation(numbers, operator)\n",
        "            abstained = torch.isclose(\n",
        "                predictions,\n",
        "                torch.tensor(-1.0).to(predictions.device),\n",
        "                rtol=0.1, atol=0.1\n",
        "            )\n",
        "\n",
        "            # Calculate overall abstention rate\n",
        "            metrics['abstention_rate'].append(abstained.float().mean().item())\n",
        "\n",
        "            # Calculate recall metrics for invalid cases\n",
        "            if invalid_mask.any():\n",
        "                invalid_recall = (abstained & invalid_mask).float().sum() / invalid_mask.float().sum()\n",
        "                metrics['invalid_recall'].append(invalid_recall.item())\n",
        "\n",
        "            # Calculate recall metrics for valid cases\n",
        "            valid_mask = ~invalid_mask\n",
        "            valid_and_not_abstained = valid_mask & (~abstained)\n",
        "\n",
        "            if valid_mask.any():\n",
        "                valid_recall = valid_and_not_abstained.float().sum() / valid_mask.float().sum()\n",
        "                metrics['valid_recall'].append(valid_recall.item())\n",
        "\n",
        "            # Track correct abstention rate\n",
        "            metrics['correct_abstention'].append(\n",
        "                (abstained & invalid_mask).float().mean().item()\n",
        "            )\n",
        "\n",
        "            # Calculate accuracy for valid cases where model didn't abstain\n",
        "            if valid_and_not_abstained.any():\n",
        "                pred_vals = predictions[valid_and_not_abstained]\n",
        "                tgt_vals = targets[valid_and_not_abstained]\n",
        "                diff = torch.abs(pred_vals - tgt_vals)\n",
        "                # Use relative threshold for accuracy calculation\n",
        "                threshold = torch.abs(tgt_vals) * 0.01 + 1e-8\n",
        "                accuracy = (diff < threshold).float().mean().item()\n",
        "                metrics['valid_accuracy'].append(accuracy)\n",
        "\n",
        "    # Ensure valid_accuracy exists in output\n",
        "    if not metrics['valid_accuracy']:\n",
        "        metrics['valid_accuracy'] = [0.0]\n",
        "\n",
        "    return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "def test_model_on_boundary_cases(\n",
        "    model: nn.Module,\n",
        "    loader: Iterator[tuple[Tensor, Tensor, Tensor]]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Test model performance specifically on boundary cases.\n",
        "\n",
        "    Args:\n",
        "        model: Neural network model for arithmetic operations\n",
        "        loader: DataLoader providing batches of (numbers, operator, targets)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing performance metrics:\n",
        "        - invalid_recall: Recall rate for invalid computation cases\n",
        "        - valid_recall: Recall rate for valid computation cases\n",
        "        - correct_abstentions: Rate of correctly abstaining on invalid cases\n",
        "        - incorrect_abstentions: Rate of incorrectly abstaining on valid cases\n",
        "        - accuracy: Accuracy on valid cases when not abstaining\n",
        "    \"\"\"\n",
        "    metrics = defaultdict(list)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            numbers, operator, targets = batch\n",
        "            predictions = model(numbers, operator)\n",
        "\n",
        "            # Ensure consistent shapes\n",
        "            predictions = predictions.squeeze()\n",
        "            targets = targets.squeeze()\n",
        "\n",
        "            # Identify invalid computations and abstentions\n",
        "            invalid_mask = is_invalid_computation(numbers, operator)\n",
        "            abstained = torch.isclose(\n",
        "                predictions,\n",
        "                torch.tensor(-1.0).to(predictions.device),\n",
        "                rtol=0.1,\n",
        "                atol=0.1\n",
        "            )\n",
        "\n",
        "            # Calculate recall metrics for invalid cases\n",
        "            if invalid_mask.any():\n",
        "                invalid_recall = (abstained & invalid_mask).float().sum() / invalid_mask.float().sum()\n",
        "                metrics['invalid_recall'].append(invalid_recall.item())\n",
        "\n",
        "            # Calculate recall metrics for valid cases\n",
        "            valid_mask = ~invalid_mask\n",
        "            if valid_mask.any():\n",
        "                valid_and_not_abstained = valid_mask & (~abstained)\n",
        "                valid_recall = valid_and_not_abstained.float().sum() / valid_mask.float().sum()\n",
        "                metrics['valid_recall'].append(valid_recall.item())\n",
        "\n",
        "            # Track abstention metrics\n",
        "            metrics['correct_abstentions'].append(\n",
        "                (abstained & invalid_mask).float().mean().item()\n",
        "            )\n",
        "            metrics['incorrect_abstentions'].append(\n",
        "                (abstained & ~invalid_mask).float().mean().item()\n",
        "            )\n",
        "\n",
        "            # Calculate accuracy for valid cases where model didn't abstain\n",
        "            valid_and_not_abstained = valid_mask & (~abstained)\n",
        "            if valid_and_not_abstained.any():\n",
        "                pred_vals = predictions[valid_and_not_abstained]\n",
        "                tgt_vals = targets[valid_and_not_abstained]\n",
        "\n",
        "                diff = torch.abs(pred_vals - tgt_vals)\n",
        "                # Use relative threshold for accuracy calculation\n",
        "                threshold = torch.abs(tgt_vals) * 0.01 + 1e-8\n",
        "                accuracy = (diff < threshold).float().mean().item()\n",
        "                metrics['accuracy'].append(accuracy)\n",
        "\n",
        "    return {k: np.mean(v) for k, v in metrics.items()}\n",
        "def run_robustness_experiments(\n",
        "    models: Dict[str, nn.Module],\n",
        "    save_dir: str = 'robustness_results',\n",
        "    model_names: Optional[List[str]] = None,\n",
        "    test_loader = Any\n",
        ") -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Run robustness experiments on multiple models with various noise levels.\n",
        "\n",
        "    Args:\n",
        "        models: Dictionary mapping model names to model instances\n",
        "        save_dir: Directory to save results and visualizations\n",
        "        model_names: Optional list of model names to test (defaults to all models in dict)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing test results for boundary and noise experiments\n",
        "    \"\"\"\n",
        "    # Create output directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, 'plots'), exist_ok=True)\n",
        "\n",
        "    # Validate model names\n",
        "    if model_names is None:\n",
        "        model_names = list(models.keys())\n",
        "    elif not all(name in models for name in model_names):\n",
        "        raise ValueError(\"All model_names must be keys in models dict\")\n",
        "\n",
        "    # Initialize results structure\n",
        "    results = {\n",
        "        # 'boundary_test': {},\n",
        "        'noise_test': {}\n",
        "    }\n",
        "\n",
        "    # Define noise levels for testing\n",
        "    noise_levels = [\n",
        "        0.5, 1.0, 2.0, 5.0, 10.0, 25.0, 50.0, 100.0,\n",
        "        200.0, 250.0, 300.0, 500.0, 1000.0, 2000.0\n",
        "    ]\n",
        "\n",
        "    # Run experiments for each noise level\n",
        "    for noise_std in tqdm(noise_levels, desc=\"Testing noise levels\"):\n",
        "        # results['boundary_test'][noise_std] = {}\n",
        "        results['noise_test'][noise_std] = {}\n",
        "\n",
        "        for name in model_names:\n",
        "            try:\n",
        "                model = models[name]\n",
        "                metrics = test_model_with_noise(model, test_loader, noise_std)\n",
        "\n",
        "                # # Store boundary test results\n",
        "                # results['boundary_test'][noise_std][name] = {\n",
        "                #     'correct_abstentions': metrics['correct_abstention'],\n",
        "                #     'invalid_recall': metrics.get('invalid_recall', 0.0)\n",
        "                # }\n",
        "\n",
        "                # Store noise test results\n",
        "                results['noise_test'][noise_std][name] = {\n",
        "                    'abstention_rate': metrics['abstention_rate'],\n",
        "                    'valid_accuracy': metrics['valid_accuracy'],\n",
        "                    'invalid_recall': metrics.get('invalid_recall', 0.0)\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nWarning: Error testing {name} with noise {noise_std}: {str(e)}\")\n",
        "                # Initialize empty results for failed tests\n",
        "                # results['boundary_test'][noise_std][name] = {\n",
        "                #     'correct_abstentions': 0.0,\n",
        "                #     'invalid_recall': 0.0\n",
        "                # }\n",
        "                results['noise_test'][noise_std][name] = {\n",
        "                    'abstention_rate': 0.0,\n",
        "                    'valid_accuracy': 0.0,\n",
        "                    'invalid_recall': 0.0\n",
        "                }\n",
        "\n",
        "    # Save results to file\n",
        "    try:\n",
        "        results_path = os.path.join(save_dir, 'robustness_results.json')\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'results': results,\n",
        "                'metadata': {\n",
        "                    'model_names': model_names,\n",
        "                    'noise_levels': noise_levels\n",
        "                }\n",
        "            }, f, indent=2)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nWarning: Could not save results: {str(e)}\")\n",
        "\n",
        "    # Generate visualizations if multiple models are present\n",
        "    if model_names:\n",
        "        try:\n",
        "            create_robustness_visualizations(results, save_dir, model_names=model_names)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nWarning: Could not create visualizations: {str(e)}\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_robustness_visualizations(\n",
        "    results: Dict[str, Dict],\n",
        "    save_dir: str,\n",
        "    model_names: Optional[List[str]] = None\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary containing experiment results\n",
        "        save_dir: Directory to save visualization files\n",
        "        model_names: Optional list of model names to include in visualizations\n",
        "\n",
        "    Returns:\n",
        "        True if visualizations were created successfully\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Get model names from results if not provided\n",
        "    if model_names is None:\n",
        "        model_names = list(next(iter(results['boundary_test'].values())).keys())\n",
        "\n",
        "    # Set up matplotlib style\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    base_params = {\n",
        "        'font.family': 'sans-serif',\n",
        "        'font.size': 12,\n",
        "        'axes.labelsize': 14,\n",
        "        'axes.titlesize': 14,\n",
        "        'legend.fontsize': 12,\n",
        "        'axes.spines.top': False,\n",
        "        'axes.spines.right': False,\n",
        "        'axes.grid': True,\n",
        "        'grid.alpha': 0.3,\n",
        "    }\n",
        "\n",
        "    # Define color palette for consistent visualization\n",
        "    colors = ['#4878D0', '#EE854A', '#6ACC64', '#9467BD', '#FF7F0E', '#2CA02C']\n",
        "\n",
        "    if len(model_names) > 1:\n",
        "        # Generate boundary test visualization\n",
        "        # _create_boundary_test_plot(\n",
        "        #     results, model_names, colors, base_params, save_dir\n",
        "        # )\n",
        "\n",
        "      #  Generate progressive noise test visualization\n",
        "        _create_noise_test_plot(\n",
        "            results, model_names, colors, base_params, save_dir\n",
        "        )\n",
        "\n",
        "        # Generate invalid recall visualization\n",
        "        _create_invalid_recall_plot(\n",
        "            results, model_names, colors, base_params, save_dir\n",
        "        )\n",
        "    else:\n",
        "        # Save metrics as JSON for single model case\n",
        "        metrics_path = os.path.join(save_dir, f'{model_names[0]}_robustness_metrics.json')\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'boundary_test': results['boundary_test'],\n",
        "                'noise_test': results['noise_test']\n",
        "            }, f, indent=2)\n",
        "\n",
        "    return True\n",
        "\n",
        "# def _create_boundary_test_plot(\n",
        "#     results: Dict[str, Dict],\n",
        "#     model_names: List[str],\n",
        "#     colors: List[str],\n",
        "#     base_params: Dict[str, Any],\n",
        "#     save_dir: str\n",
        "# ) -> None:\n",
        "#     \"\"\"Helper function to create boundary test visualization.\"\"\"\n",
        "#     plt.rcParams.update({**base_params, 'figure.figsize': (8, 6)})\n",
        "#     fig = plt.figure(constrained_layout=True)\n",
        "#     ax = fig.add_subplot(111)\n",
        "\n",
        "#     noise_levels = sorted(results['boundary_test'].keys())\n",
        "#     x = np.arange(len(noise_levels))\n",
        "#     width = 0.8 / len(model_names)\n",
        "\n",
        "#     for i, model_name in enumerate(model_names):\n",
        "#         try:\n",
        "#             correct_vals = [\n",
        "#                 results['boundary_test'][n][model_name]['correct_abstentions']\n",
        "#                 for n in noise_levels\n",
        "#             ]\n",
        "#             position = x + width * (i - len(model_names)/2 + 0.5)\n",
        "#             ax.bar(position, correct_vals, width,\n",
        "#                    label=model_name,\n",
        "#                    color=colors[i % len(colors)],\n",
        "#                    edgecolor='white',\n",
        "#                    linewidth=0.5)\n",
        "#         except KeyError as e:\n",
        "#             print(f\"Warning: Missing data for {model_name}: {e}\")\n",
        "#             continue\n",
        "\n",
        "#     _format_bar_plot(ax, 'Correct Abstentions at Decision Boundaries',\n",
        "#                     'Noise Level', 'Rate', noise_levels, model_names)\n",
        "\n",
        "#     plt.savefig(os.path.join(save_dir, 'boundary_test.png'),\n",
        "#                 dpi=300, bbox_inches='tight')\n",
        "#     plt.close(fig)\n",
        "\n",
        "def _create_noise_test_plot(\n",
        "    results: Dict[str, Dict],\n",
        "    model_names: List[str],\n",
        "    colors: List[str],\n",
        "    base_params: Dict[str, Any],\n",
        "    save_dir: str\n",
        ") -> None:\n",
        "    \"\"\"Helper function to create progressive noise test visualization.\"\"\"\n",
        "    plt.rcParams.update({**base_params, 'figure.figsize': (10, 7)})\n",
        "    fig = plt.figure(constrained_layout=True)\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    noise_levels = sorted(results['noise_test'].keys())\n",
        "\n",
        "    for i, model_name in enumerate(model_names):\n",
        "        try:\n",
        "            metrics_data = [results['noise_test'][n][model_name]\n",
        "                          for n in noise_levels]\n",
        "\n",
        "            # Plot accuracy\n",
        "            ax.plot(noise_levels,\n",
        "                    [m['valid_accuracy'] for m in metrics_data],\n",
        "                    label=f'{model_name} (accuracy)',\n",
        "                    color=colors[i % len(colors)],\n",
        "                    marker='s',\n",
        "                    linestyle='--',\n",
        "                    linewidth=3,\n",
        "                    markersize=8,\n",
        "                    markeredgecolor='white',\n",
        "                    markeredgewidth=2)\n",
        "\n",
        "            # Plot abstention rate\n",
        "            ax.plot(noise_levels,\n",
        "                    [m['abstention_rate'] for m in metrics_data],\n",
        "                    label=f'{model_name} (abstention)',\n",
        "                    color=colors[i % len(colors)],\n",
        "                    marker='o',\n",
        "                    linestyle='-',\n",
        "                    linewidth=2,\n",
        "                    markersize=8,\n",
        "                    markeredgecolor='white',\n",
        "                    markeredgewidth=2,\n",
        "                    alpha=0.7)\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Missing data for {model_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    _format_line_plot(ax, 'Performance Under Progressive Noise',\n",
        "                     'Noise Level', 'Rate')\n",
        "\n",
        "    plt.savefig(os.path.join(save_dir, 'progressive_noise.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "def _create_invalid_recall_plot(\n",
        "    results: Dict[str, Dict],\n",
        "    model_names: List[str],\n",
        "    colors: List[str],\n",
        "    base_params: Dict[str, Any],\n",
        "    save_dir: str\n",
        ") -> None:\n",
        "    \"\"\"Helper function to create invalid recall visualization.\"\"\"\n",
        "    plt.rcParams.update({**base_params, 'figure.figsize': (8, 6)})\n",
        "    fig = plt.figure(constrained_layout=True)\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    noise_levels = sorted(results['noise_test'].keys())\n",
        "    x = np.arange(len(noise_levels))\n",
        "    width = 0.8 / len(model_names)\n",
        "\n",
        "    for i, model_name in enumerate(model_names):\n",
        "        try:\n",
        "            invalid_recall_vals = [\n",
        "                results['noise_test'][n][model_name]['invalid_recall']\n",
        "                for n in noise_levels\n",
        "            ]\n",
        "            position = x + width * (i - len(model_names)/2 + 0.5)\n",
        "            ax.bar(position, invalid_recall_vals, width,\n",
        "                   label=model_name,\n",
        "                   color=colors[i % len(colors)],\n",
        "                   edgecolor='white',\n",
        "                   linewidth=0.5)\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Missing data for {model_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    _format_bar_plot(ax, 'Invalid Case Detection Under Progressive Noise',\n",
        "                    'Noise Level', 'Invalid Recall Rate', noise_levels, model_names)\n",
        "\n",
        "    plt.savefig(os.path.join(save_dir, 'invalid_recall.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "def _format_bar_plot(\n",
        "    ax: plt.Axes,\n",
        "    title: str,\n",
        "    xlabel: str,\n",
        "    ylabel: str,\n",
        "    xtick_labels: List[Union[str, float]],\n",
        "    model_names: List[str]\n",
        ") -> None:\n",
        "    \"\"\"Helper function to format bar plots consistently.\"\"\"\n",
        "    ax.set_title(title, pad=20)\n",
        "    ax.set_xlabel(xlabel, labelpad=10)\n",
        "    ax.set_ylabel(ylabel, labelpad=10)\n",
        "    ax.set_xticks(np.arange(len(xtick_labels)))\n",
        "    ax.set_xticklabels(xtick_labels)\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.set_yticks(np.arange(0, 1.1, 0.2))\n",
        "\n",
        "    legend_cols = min(3, len(model_names))\n",
        "    ax.legend(bbox_to_anchor=(0.5, -0.15),\n",
        "             loc='upper center',\n",
        "             ncol=legend_cols,\n",
        "             frameon=False,\n",
        "             handlelength=1.5)\n",
        "\n",
        "    width = 0.8 / len(model_names)\n",
        "    x = np.arange(len(xtick_labels))\n",
        "    ax.set_xlim(x[0] - width*2, x[-1] + width*2)\n",
        "\n",
        "def _format_line_plot(\n",
        "    ax: plt.Axes,\n",
        "    title: str,\n",
        "    xlabel: str,\n",
        "    ylabel: str\n",
        ") -> None:\n",
        "    \"\"\"Helper function to format line plots consistently.\"\"\"\n",
        "    ax.set_title(title, pad=20)\n",
        "    ax.set_xlabel(xlabel, labelpad=10)\n",
        "    ax.set_ylabel(ylabel, labelpad=10)\n",
        "    ax.grid(True, linestyle='--', alpha=0.4)\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.set_yticks(np.arange(0, 1.1, 0.2))\n",
        "    ax.legend(bbox_to_anchor=(1.02, 0.5),\n",
        "             loc='center left',\n",
        "             frameon=False,\n",
        "             handlelength=2.5,\n",
        "             borderaxespad=0)\n",
        "def evaluate_models_on_test(\n",
        "    models: List[nn.Module],\n",
        "    test_loader: DataLoader,\n",
        "    save_dir: Union[str, Path] = 'test_results',\n",
        "    model_names: Optional[List[str]] = None\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Evaluate multiple models on test data and calculate performance metrics.\n",
        "\n",
        "    Args:\n",
        "        models: List of PyTorch models to evaluate\n",
        "        test_loader: DataLoader containing test data batches\n",
        "        save_dir: Directory to save evaluation results\n",
        "        model_names: Optional list of names for the models (must match length of models)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping model names to their performance metrics\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If number of model names doesn't match number of models\n",
        "    \"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize results storage with nested defaultdict\n",
        "    results: Dict[str, Dict[str, Any]] = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    # Generate default model names if not provided\n",
        "    if model_names is None:\n",
        "        model_names = [f\"Model_{i}\" for i in range(len(models))]\n",
        "    elif len(model_names) != len(models):\n",
        "        raise ValueError(\"Number of model names must match number of models\")\n",
        "\n",
        "    # Evaluate each model separately\n",
        "    for model_idx, (model, model_name) in enumerate(zip(models, model_names)):\n",
        "        print(f\"\\nEvaluating {model_name} Model...\")\n",
        "        model.eval()\n",
        "\n",
        "        # Initialize metric tracking\n",
        "        metrics = defaultdict(float)\n",
        "        total_samples = 0\n",
        "\n",
        "        # Evaluate model on test data\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                # Unpack batch data\n",
        "                numbers, operator, targets = batch\n",
        "\n",
        "                # Get model predictions\n",
        "                predictions = model(numbers, operator)\n",
        "\n",
        "                # Identify valid and invalid computations\n",
        "                invalid_mask = is_invalid_computation(numbers, operator)\n",
        "                valid_mask = ~invalid_mask\n",
        "\n",
        "                # Check for abstentions (predictions close to abstention token)\n",
        "                abstained = torch.isclose(\n",
        "                    predictions.squeeze(),\n",
        "                    torch.tensor(-1.0).to(predictions.device),\n",
        "                    rtol=0.1,\n",
        "                    atol=0.1\n",
        "                )\n",
        "\n",
        "                # Update batch statistics\n",
        "                batch_size = len(predictions)\n",
        "                total_samples += batch_size\n",
        "\n",
        "                # Calculate accuracy for valid computations where model didn't abstain\n",
        "                valid_and_not_abstained = valid_mask & (~abstained)\n",
        "                if valid_and_not_abstained.any():\n",
        "                    pred_vals = predictions[valid_and_not_abstained]\n",
        "                    tgt_vals = targets[valid_and_not_abstained]\n",
        "\n",
        "                    # Compare predictions with targets using relative threshold\n",
        "                    diff = torch.abs(pred_vals - tgt_vals)\n",
        "                    threshold = torch.abs(tgt_vals) * 0.01 + 1e-8  # 1% relative threshold\n",
        "                    metrics['correct_valid'] += (diff < threshold).sum().item()\n",
        "\n",
        "                # Update running metrics\n",
        "                metrics['total_valid'] += valid_mask.sum().item()\n",
        "                metrics['total_invalid'] += invalid_mask.sum().item()\n",
        "                metrics['total_abstained'] += abstained.sum().item()\n",
        "                metrics['correct_abstentions'] += (abstained & invalid_mask).sum().item()\n",
        "                metrics['loss'] += F.mse_loss(predictions, targets).item() * batch_size\n",
        "\n",
        "        # Calculate final metrics as percentages\n",
        "        final_metrics = {\n",
        "            'accuracy': (metrics['correct_valid'] / max(metrics['total_valid'], 1)),\n",
        "            'abstention_rate': (metrics['total_abstained'] / total_samples),\n",
        "            'abstention_precision': (metrics['correct_abstentions'] / max(metrics['total_abstained'], 1)),\n",
        "            'abstention_recall': (metrics['correct_abstentions'] / max(metrics['total_invalid'], 1)),\n",
        "            'loss': metrics['loss'] / total_samples\n",
        "        }\n",
        "\n",
        "        # Store results for this model\n",
        "        results[model_name].update(final_metrics)\n",
        "\n",
        "        # Print evaluation results\n",
        "        print(f\"  Test Loss:            {final_metrics['loss']:.4f}\")\n",
        "        print(f\"  Test Accuracy:        {final_metrics['accuracy'] * 100:.4f}%\")\n",
        "        print(f\"  Abstention Rate:      {final_metrics['abstention_rate'] * 100:.4f}%\")\n",
        "        print(f\"  Abstention Precision: {final_metrics['abstention_precision'] * 100:.4f}%\")\n",
        "        print(f\"  Abstention Recall:    {final_metrics['abstention_recall'] * 100:.4f}%\")\n",
        "\n",
        "        # Save individual model metrics\n",
        "        metrics_path = os.path.join(save_dir, f'{model_name}_metrics.json')\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(final_metrics, f, indent=2)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ggmAlxRN7k"
      },
      "outputs": [],
      "source": [
        "class OODEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluator for testing model behavior on out-of-distribution inputs.\n",
        "\n",
        "    Handles evaluation of models on various test cases including:\n",
        "    - Number format variations\n",
        "    - Novel operators\n",
        "    - Cross-boundary cases\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_file_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the OOD evaluator.\n",
        "\n",
        "        Args:\n",
        "            test_file_path: Path to JSON file containing test cases\n",
        "        \"\"\"\n",
        "        with open(test_file_path, 'r') as f:\n",
        "            self.test_data = json.load(f)\n",
        "\n",
        "        # Model configuration constants\n",
        "        self.RTOL: float = 0.1\n",
        "        self.ATOL: float = 0.1\n",
        "\n",
        "        # Mapping of operators to their indices\n",
        "        # Novel operators are mapped to '@' (2) by default\n",
        "        self.op_to_idx: Dict[str, int] = {\n",
        "            '+': 0,\n",
        "            '-': 1,\n",
        "            '@': 2,\n",
        "            '#': 2,  # Novel operators\n",
        "            '$': 2,\n",
        "            '&': 2,\n",
        "            '^': 2,\n",
        "            '%': 2\n",
        "        }\n",
        "\n",
        "    def preprocess_input(self, arg1: str, arg2: str) -> Tuple[Optional[float], Optional[float]]:\n",
        "        \"\"\"\n",
        "        Convert string number representations to floats.\n",
        "\n",
        "        Args:\n",
        "            arg1: First number as string\n",
        "            arg2: Second number as string\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (float, float) if conversion successful, (None, None) otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            num1 = float(arg1)\n",
        "            num2 = float(arg2)\n",
        "            return num1, num2\n",
        "        except ValueError:\n",
        "            print(f\"Error converting numbers: {arg1}, {arg2}\")\n",
        "            return None, None\n",
        "    def check_abstain_label_distribution(self, test_cases: List[Dict[str, Any]]) -> float:\n",
        "      \"\"\"\n",
        "      Calculate the fraction of test cases where 'Should Abstain?' is False.\n",
        "\n",
        "      Args:\n",
        "          test_cases: List of test cases, each containing the 'Should Abstain?' label.\n",
        "\n",
        "      Returns:\n",
        "          Fraction of cases where 'Should Abstain?' is False.\n",
        "      \"\"\"\n",
        "      total_cases = len(test_cases)\n",
        "      if total_cases == 0:\n",
        "          return 0.0  # Avoid division by zero if the test set is empty\n",
        "\n",
        "      non_abstain_cases = sum(1 for case in test_cases if not case['Should Abstain?'])\n",
        "      return non_abstain_cases / total_cases\n",
        "\n",
        "\n",
        "    def check_abstention(self, output: torch.Tensor) -> bool:\n",
        "        \"\"\"\n",
        "        Check if model output indicates abstention.\n",
        "\n",
        "        Args:\n",
        "            output: Model output tensor\n",
        "\n",
        "        Returns:\n",
        "            True if output is close to abstention token, False otherwise\n",
        "        \"\"\"\n",
        "        return torch.isclose(\n",
        "            output.squeeze(),\n",
        "            torch.tensor(-1.0,\n",
        "                        dtype=torch.float32,\n",
        "                        device=output.device),\n",
        "            rtol=self.RTOL,\n",
        "            atol=self.ATOL\n",
        "        ).item()\n",
        "\n",
        "    def evaluate_model(self,\n",
        "                      model: nn.Module,\n",
        "                      test_cases: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate a model on a set of test cases.\n",
        "\n",
        "        Args:\n",
        "            model: PyTorch model to evaluate\n",
        "            test_cases: List of test cases, each containing arguments and expected behavior\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of evaluation metrics including accuracy and abstention metrics\n",
        "        \"\"\"\n",
        "        correct = 0\n",
        "        total = len(test_cases)\n",
        "        results = defaultdict(int)\n",
        "\n",
        "        for case in test_cases:\n",
        "            # Preprocess inputs\n",
        "            arg1, arg2 = self.preprocess_input(case['Argument 1'], case['Argument 2'])\n",
        "            if arg1 is None or arg2 is None:\n",
        "                continue\n",
        "\n",
        "            # Prepare model inputs\n",
        "            numbers = torch.tensor([[arg1, arg2]], dtype=torch.float32)\n",
        "            op_idx = self.op_to_idx.get(case['Operator'], 2)\n",
        "            operator = torch.tensor([op_idx], dtype=torch.long)\n",
        "\n",
        "            # Get model prediction\n",
        "            with torch.no_grad():\n",
        "                output = model(numbers, operator)\n",
        "                predicted_abstain = self.check_abstention(output)\n",
        "\n",
        "            # Update metrics\n",
        "            correct += (predicted_abstain == case['Should Abstain?'])\n",
        "            self._update_results_tracking(results, case, predicted_abstain)\n",
        "\n",
        "        # Calculate and return final metrics\n",
        "        return self._calculate_metrics(correct, total, results, test_cases)\n",
        "\n",
        "    def evaluate_all_models(self,\n",
        "                          models: Dict[str, nn.Module]) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
        "        \"\"\"\n",
        "        Evaluate multiple models on all test sets.\n",
        "\n",
        "        Args:\n",
        "            models: Dictionary mapping model names to PyTorch models\n",
        "\n",
        "        Returns:\n",
        "            Nested dictionary of results for each model and test type\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            model.eval()\n",
        "            results[model_name] = {\n",
        "                'number_format': self.evaluate_model(\n",
        "                    model, self.test_data['number_format_tests']\n",
        "                ),\n",
        "                'novel_operator': self.evaluate_model(\n",
        "                    model, self.test_data['novel_operator_tests']\n",
        "                ),\n",
        "                'cross_boundary': self.evaluate_model(\n",
        "                    model, self.test_data['cross_boundary_tests']\n",
        "                )\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _update_results_tracking(self,\n",
        "                               results: Dict[str, int],\n",
        "                               case: Dict[str, Any],\n",
        "                               predicted_abstain: bool) -> None:\n",
        "        \"\"\"\n",
        "        Update tracking dictionary with results from a single test case.\n",
        "\n",
        "        Args:\n",
        "            results: Dictionary tracking result counts\n",
        "            case: Current test case\n",
        "            predicted_abstain: Whether model predicted abstention\n",
        "        \"\"\"\n",
        "        results['total_cases'] += 1\n",
        "        if case['Should Abstain?']:\n",
        "            results['should_abstain'] += 1\n",
        "            if predicted_abstain:\n",
        "                results['correct_abstentions'] += 1\n",
        "        else:\n",
        "            results['should_not_abstain'] += 1\n",
        "            if not predicted_abstain:\n",
        "                results['correct_non_abstentions'] += 1\n",
        "\n",
        "    def _calculate_metrics(self,\n",
        "                         correct: int,\n",
        "                         total: int,\n",
        "                         results: Dict[str, int],\n",
        "                         test_cases: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calculate final metrics from results tracking.\n",
        "\n",
        "        Args:\n",
        "            correct: Total number of correct predictions\n",
        "            total: Total number of test cases\n",
        "            results: Dictionary tracking result counts\n",
        "            test_cases: List of all test cases\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of calculated metrics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'overall_accuracy': correct / total,\n",
        "            'abstention_precision': results['correct_abstentions'] / max(\n",
        "                1, results['should_abstain']\n",
        "            ),\n",
        "            'abstention_recall': results['correct_abstentions'] / max(\n",
        "                1, sum(1 for c in test_cases if c['Should Abstain?'])\n",
        "            ),\n",
        "            'total_cases': total\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_models(\n",
        "    models: Dict[str, nn.Module],\n",
        "    test_file: str = 'ood_test_set.json'\n",
        ") -> Dict[str, Dict[str, Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Main evaluation function for OOD testing.\n",
        "\n",
        "    Args:\n",
        "        models: Dictionary mapping model names to PyTorch models\n",
        "        test_file: Path to JSON file containing test cases\n",
        "\n",
        "    Returns:\n",
        "        Nested dictionary containing evaluation results for all models\n",
        "    \"\"\"\n",
        "    evaluator = OODEvaluator(test_file)\n",
        "    return evaluator.evaluate_all_models(models)\n",
        "\n",
        "\n",
        "def print_evaluation_results(\n",
        "    results: Dict[str, Dict[str, Dict[str, float]]]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Print formatted evaluation results.\n",
        "\n",
        "    Args:\n",
        "        results: Nested dictionary of evaluation results\n",
        "    \"\"\"\n",
        "    for model_name, model_results in results.items():\n",
        "        print(f\"\\nResults for {model_name}:\")\n",
        "        for test_type, metrics in model_results.items():\n",
        "            print(f\"\\n{test_type.replace('_', ' ').title()}:\")\n",
        "            for metric, value in metrics.items():\n",
        "                print(f\"{metric.replace('_', ' ').title()}: {value:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "91_IdlRyRSDA",
        "outputId": "886b22e7-c73f-4ec8-8779-7eb3a0a34295"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Configuration for experiment parameters.\"\"\"\n",
        "    analysis_freq: int\n",
        "    seed: int = 16\n",
        "    noise_std: float = 0.1\n",
        "    epochs: int = 72\n",
        "    batch_size: int = 32\n",
        "    ood_test_file: str = 'ood_test_set.json'\n",
        "    val_freq: int = 1\n",
        "\n",
        "class ExperimentDownloadManager:\n",
        "    \"\"\"Manages experiment result files and downloads.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str = \"experiment_results\") -> None:\n",
        "        \"\"\"\n",
        "        Initialize the download manager.\n",
        "\n",
        "        Args:\n",
        "            base_dir: Base directory for storing experiment results\n",
        "        \"\"\"\n",
        "        self.base_dir: str = base_dir\n",
        "        self.files_to_download: Set[str] = set()\n",
        "\n",
        "    def clear_old_results(self) -> None:\n",
        "        \"\"\"Clear previous results and create fresh directory structure.\"\"\"\n",
        "        if os.path.exists(self.base_dir):\n",
        "            shutil.rmtree(self.base_dir)\n",
        "\n",
        "        # Create directory structure\n",
        "        subdirs = ['plots', 'metrics', 'landscapes', 'robustness']\n",
        "        os.makedirs(self.base_dir)\n",
        "        for subdir in subdirs:\n",
        "            os.makedirs(os.path.join(self.base_dir, subdir))\n",
        "\n",
        "    def add_file(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Track a file for downloading.\n",
        "\n",
        "        Args:\n",
        "            filepath: Path to file to be tracked\n",
        "        \"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            self.files_to_download.add(filepath)\n",
        "\n",
        "    def zip_and_download(self, experiment_name: str) -> None:\n",
        "        \"\"\"\n",
        "        Create and download a zip archive of experiment results.\n",
        "\n",
        "        Args:\n",
        "            experiment_name: Base name for the zip file\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        zip_filename = f'{experiment_name}_{timestamp}.zip'\n",
        "        zip_path = os.path.join(self.base_dir, zip_filename)\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file_path in tqdm(self.files_to_download, desc=\"Adding files\"):\n",
        "                arcname = os.path.relpath(file_path, self.base_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "        files.download(zip_path)\n",
        "\n",
        "\n",
        "class ExperimentRunner:\n",
        "    \"\"\"Manages and runs the entire experiment.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ExperimentConfig) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the experiment runner.\n",
        "\n",
        "        Args:\n",
        "            config: Experiment configuration parameters\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.download_mgr = ExperimentDownloadManager()\n",
        "        self.download_mgr.clear_old_results()\n",
        "\n",
        "        # Set random seed\n",
        "        self.seed = set_seed(config.seed)\n",
        "\n",
        "        # Initialize metrics storage\n",
        "        self.computational_metrics: Dict[str, Any] = {}\n",
        "        os.makedirs(os.path.join(self.download_mgr.base_dir, 'ood_tests'), exist_ok=True)\n",
        "\n",
        "    def _run_ood_evaluation(\n",
        "        self,\n",
        "        models: List[nn.Module],\n",
        "        model_names: List[str]\n",
        "    ) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
        "        \"\"\"\n",
        "        Run OOD evaluations on all models.\n",
        "\n",
        "        Args:\n",
        "            models: List of trained models\n",
        "            model_names: Names of the models\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing OOD evaluation results\n",
        "        \"\"\"\n",
        "        print(\"\\nRunning OOD evaluations...\")\n",
        "\n",
        "        model_dict = dict(zip(model_names, models))\n",
        "\n",
        "        # Run OOD evaluation\n",
        "        ood_results = evaluate_models(\n",
        "            models=model_dict,\n",
        "            test_file=self.config.ood_test_file\n",
        "        )\n",
        "\n",
        "        # Save OOD results\n",
        "        ood_save_path = os.path.join(\n",
        "            self.download_mgr.base_dir,\n",
        "            'ood_tests',\n",
        "            'ood_results.json'\n",
        "        )\n",
        "        with open(ood_save_path, 'w') as f:\n",
        "            json.dump(ood_results, f, indent=2)\n",
        "        self.download_mgr.add_file(ood_save_path)\n",
        "\n",
        "        # Create visualizations for OOD results\n",
        "       # self._create_ood_visualizations(ood_results, model_names)\n",
        "\n",
        "        return ood_results\n",
        "\n",
        "    def _create_ood_visualizations(\n",
        "        self,\n",
        "        ood_results: Dict[str, Dict[str, Dict[str, float]]],\n",
        "        model_names: List[str]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Create visualizations for OOD test results.\n",
        "\n",
        "        Args:\n",
        "            ood_results: Results from OOD evaluation\n",
        "            model_names: Names of the models\n",
        "        \"\"\"\n",
        "        viz_dir = os.path.join(self.download_mgr.base_dir, 'ood_tests', 'visualizations')\n",
        "        os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "        # Create log comparison plots\n",
        "        create_log_comparison_plots(ood_results, viz_dir)\n",
        "\n",
        "        # Track visualization files\n",
        "        for file in glob.glob(os.path.join(viz_dir, '*.png')):\n",
        "            self.download_mgr.add_file(file)\n",
        "\n",
        "\n",
        "\n",
        "    def _initialize_models(self) -> tuple[List[nn.Module], List[str]]:\n",
        "        \"\"\"\n",
        "        Initialize all models for the experiment.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (models, model_names)\n",
        "        \"\"\"\n",
        "        decay_control = ArithmeticNet()\n",
        "        base_model = ArithmeticNet()\n",
        "        pgd_model = ArithmeticNet()\n",
        "        adv_model = ArithmeticNet()  # New adversarial training model\n",
        "        full_pgd = ArithmeticNet()\n",
        "\n",
        "        models = [base_model, adv_model, decay_control, full_pgd, pgd_model]\n",
        "        model_names = [\"base_adam\", \"input_space_adv\", \"decay_control\", \"full_pgd\", \"pgd\"]\n",
        "\n",
        "        return models, model_names\n",
        "\n",
        "    def _get_data_loaders(self) -> tuple[Any, Any, Any]:\n",
        "        \"\"\"\n",
        "        Create data loaders for training, validation, and testing.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (train_loader, val_loader, test_loader)\n",
        "        \"\"\"\n",
        "        with open('abstention_dataset.json', 'r') as f:\n",
        "          dataset_dict = json.load(f)\n",
        "\n",
        "        noise_config = {\n",
        "            'enabled': True,\n",
        "            'std': self.config.noise_std\n",
        "        }\n",
        "\n",
        "        return ArithmeticDataset.get_train_val_test_loaders(\n",
        "            dataset_dict,\n",
        "            batch_size=self.config.batch_size,\n",
        "            noise_config=noise_config\n",
        "        )\n",
        "\n",
        "    def _get_trainer(self, model: nn.Module, name: str) -> Any:\n",
        "        \"\"\"\n",
        "        Get appropriate trainer based on model type.\n",
        "\n",
        "        Args:\n",
        "            model: Model to train\n",
        "            name: Model name/type\n",
        "\n",
        "        Returns:\n",
        "            Trainer instance\n",
        "        \"\"\"\n",
        "        if name == \"pgd\":\n",
        "            return EfficientPGDTrainer(\n",
        "                model=model,\n",
        "                optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
        "                k=2,\n",
        "                sample_ratio=0.3\n",
        "            )\n",
        "        elif name == \"full_pgd\":\n",
        "            return PGDTrainer(\n",
        "                model=model,\n",
        "                optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
        "                k=2 # WAS 3 TESTING K =2\n",
        "            )\n",
        "        elif name == \"input_space_adv\":  # New case for adversarial trainer\n",
        "            return InputSpaceAdversarialTrainer(\n",
        "               model=model,\n",
        "               optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
        "               epsilon=0.1,  # Small perturbation magnitude\n",
        "               steps=2,      # Number of PGD steps\n",
        "               adv_ratio=0.10  # Was 5% adversarial examples before, trying 10 first\n",
        "          )\n",
        "        elif name == \"decay_control\":\n",
        "            return BaseTrainer(\n",
        "                model,\n",
        "                optimizer=torch.optim.Adam(\n",
        "                    model.parameters(),\n",
        "                    weight_decay=1e-4,\n",
        "                    lr=0.001\n",
        "                )\n",
        "            )\n",
        "        elif name == \"base_adam\":\n",
        "            return BaseTrainer(\n",
        "                model,\n",
        "                optimizer=torch.optim.Adam(\n",
        "                    model.parameters(),\n",
        "                    lr=0.001\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {name}\")\n",
        "\n",
        "    def run(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run the full experiment pipeline.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all experiment results\n",
        "        \"\"\"\n",
        "        # Initialize models and framework\n",
        "        models, model_names = self._initialize_models()\n",
        "        train_loader, val_loader, test_loader = self._get_data_loaders()\n",
        "        trainers = [self._get_trainer(model, name)\n",
        "                    for model, name in zip(models, model_names)]\n",
        "\n",
        "\n",
        "        # Initialize framework\n",
        "        framework = UnifiedTrainingFramework(\n",
        "            models=models,\n",
        "            save_dir=self.download_mgr.base_dir,\n",
        "            training_config=self.config,\n",
        "            model_names=model_names.copy(),\n",
        "            trainers=trainers\n",
        "        )\n",
        "\n",
        "        # Train models and collect metrics\n",
        "        all_metrics = self._train_models(\n",
        "            models, model_names, framework,\n",
        "            train_loader, val_loader\n",
        "        )\n",
        "\n",
        "        # Run evaluations\n",
        "        test_results, boundary_results = self._run_evaluations(\n",
        "            models, model_names, test_loader\n",
        "        )\n",
        "\n",
        "        # Run robustness experiments\n",
        "        robustness_results = self._run_robustness_tests(\n",
        "            models, model_names, test_loader\n",
        "        )\n",
        "\n",
        "        # Run OOD evaluations\n",
        "        ood_results = self._run_ood_evaluation(models, model_names)\n",
        "\n",
        "        # Compile and save final results\n",
        "        results = self._save_results(\n",
        "            test_results=test_results,\n",
        "            boundary_results=boundary_results,\n",
        "            robustness_results=robustness_results,\n",
        "            ood_results=ood_results,\n",
        "            all_metrics=all_metrics\n",
        "        )\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def _train_models(\n",
        "        self,\n",
        "        models: List[nn.Module],\n",
        "        model_names: List[str],\n",
        "        framework: Any,\n",
        "        train_loader: Any,\n",
        "        val_loader: Any\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Train all models and collect metrics.\"\"\"\n",
        "        all_metrics = {}\n",
        "\n",
        "        for idx, (model, name) in enumerate(zip(models, model_names)):\n",
        "            trainer = self._get_trainer(model, name)\n",
        "            print(f\"Training {(name)} model:\")\n",
        "            print(f\"Trainer settings:\\n {trainer.__dict__}\")\n",
        "            model_metrics = framework.train_single_model(\n",
        "                model_idx=idx,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                epochs=self.config.epochs,\n",
        "                analyze_every=self.config.analysis_freq\n",
        "            )\n",
        "\n",
        "            all_metrics[name] = model_metrics\n",
        "            if hasattr(framework.trainers[idx], \"tracker\"):\n",
        "                self.computational_metrics[name] = framework.trainers[idx].tracker.get_computational_metrics()\n",
        "\n",
        "            # Save model checkpoint\n",
        "            model_path = os.path.join(self.download_mgr.base_dir, f'model_{name}.pt')\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            self.download_mgr.add_file(model_path)\n",
        "\n",
        "        return all_metrics\n",
        "\n",
        "    def _run_evaluations(\n",
        "        self,\n",
        "        models: List[nn.Module],\n",
        "        model_names: List[str],\n",
        "        test_loader: Any\n",
        "    ) -> tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "        \"\"\"Run model evaluations on test and boundary datasets.\"\"\"\n",
        "        # Create boundary test loader\n",
        "        boundary_loader = create_boundary_test_loader(\n",
        "            test_loader,\n",
        "            num_samples=1000,\n",
        "        )\n",
        "\n",
        "        metrics_dir = os.path.join(self.download_mgr.base_dir, 'metrics')\n",
        "\n",
        "        # Run evaluations\n",
        "        print(\"Evaluating models on standard test set:\")\n",
        "        test_results = evaluate_models_on_test(\n",
        "            models=models,\n",
        "            test_loader=test_loader,\n",
        "            save_dir=metrics_dir,\n",
        "            model_names=model_names\n",
        "        )\n",
        "\n",
        "        print(\"Evaluating models on boundary challenge set:\")\n",
        "\n",
        "        boundary_results = evaluate_models_on_test(\n",
        "            models=models,\n",
        "            test_loader=boundary_loader,\n",
        "            save_dir=metrics_dir,\n",
        "            model_names=model_names\n",
        "        )\n",
        "\n",
        "        return test_results, boundary_results\n",
        "\n",
        "    def _run_robustness_tests(\n",
        "        self,\n",
        "        models: List[nn.Module],\n",
        "        model_names: List[str],\n",
        "        test_loader: Any\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Run robustness experiments on models.\"\"\"\n",
        "        model_dict = dict(zip(model_names, models))\n",
        "\n",
        "        return run_robustness_experiments(\n",
        "            models=model_dict,\n",
        "            save_dir=os.path.join(self.download_mgr.base_dir, 'robustness'),\n",
        "            model_names=model_names,\n",
        "            test_loader = test_loader\n",
        "        )\n",
        "\n",
        "    def _save_results(\n",
        "        self,\n",
        "        test_results: Dict[str, Any],\n",
        "        boundary_results: Dict[str, Any],\n",
        "        robustness_results: Dict[str, Any],\n",
        "        ood_results: Dict[str, Any],\n",
        "        all_metrics: Dict[str, Any]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Save all results including OOD evaluation.\"\"\"\n",
        "        from google.colab import drive\n",
        "\n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        final_results = {\n",
        "            'test_results': test_results,\n",
        "            'boundary_results': boundary_results,\n",
        "            'robustness_results': robustness_results,\n",
        "            'ood_results': ood_results,\n",
        "            'training_metrics': all_metrics,\n",
        "            'computational_metrics': self.computational_metrics,\n",
        "            'experiment_config': {\n",
        "                'seed': self.seed,\n",
        "                'noise_std': self.config.noise_std,\n",
        "                'epochs': self.config.epochs,\n",
        "                'ood_test_file': self.config.ood_test_file\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save results locally first\n",
        "        results_path = os.path.join(\n",
        "            self.download_mgr.base_dir,\n",
        "            'final_results.json'\n",
        "        )\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(final_results, f, indent=2)\n",
        "        self.download_mgr.add_file(results_path)\n",
        "\n",
        "        # Track generated files\n",
        "        for root, _, files in os.walk(self.download_mgr.base_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(('.png', '.jpg', '.json', '.pt')):\n",
        "                    self.download_mgr.add_file(os.path.join(root, file))\n",
        "\n",
        "        # Create archive name\n",
        "        experiment_name = (\n",
        "            f\"abstention_experiment_s{self.seed}_\"\n",
        "            f\"n{self.config.noise_std}\"\n",
        "        )\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        zip_filename = f'{experiment_name}_{timestamp}.zip'\n",
        "        zip_path = os.path.join(self.download_mgr.base_dir, zip_filename)\n",
        "\n",
        "        # Create zip file\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file_path in tqdm(self.download_mgr.files_to_download, desc=\"Adding files\"):\n",
        "                arcname = os.path.relpath(file_path, self.download_mgr.base_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "        # Save to Google Drive\n",
        "        drive_folder = '/content/drive/MyDrive/abstention_experiments'\n",
        "        os.makedirs(drive_folder, exist_ok=True)\n",
        "        drive_path = os.path.join(drive_folder, zip_filename)\n",
        "        shutil.copy2(zip_path, drive_path)\n",
        "        print(f\"\\nSaved experiment archive to Google Drive: {drive_path}\")\n",
        "\n",
        "        # Download to local machine\n",
        "        self.download_mgr.zip_and_download(experiment_name)\n",
        "\n",
        "        return final_results\n",
        "\n",
        "\n",
        "def run_experiment(\n",
        "    analysis_freq: int,\n",
        "    seed: int = 16,\n",
        "    noise_std: float = 0.1,\n",
        "    epochs: int = 72\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Main entry point for running experiments.\n",
        "\n",
        "    Args:\n",
        "        analysis_freq: Frequency of analysis during training\n",
        "        seed: Random seed\n",
        "        noise_std: Standard deviation of noise\n",
        "        epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing all experiment results\n",
        "    \"\"\"\n",
        "    config = ExperimentConfig(\n",
        "        analysis_freq=analysis_freq,\n",
        "        seed=seed,\n",
        "        noise_std=noise_std,\n",
        "        epochs=epochs,\n",
        "        ood_test_file='/content/ood_test_set.json'\n",
        "    )\n",
        "\n",
        "    runner = ExperimentRunner(config)\n",
        "    return runner.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_experiment(\n",
        "        analysis_freq=80,\n",
        "        seed= 1,\n",
        "        noise_std=0.3,\n",
        "        epochs=80\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fm4z62TaSJTw"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class VisualizationConfig:\n",
        "    \"\"\"Configuration for visualization styling.\"\"\"\n",
        "    color_map: Dict[str, str]\n",
        "    figsize: Tuple[int, int] = (12, 6)\n",
        "    grid_alpha: float = 0.2\n",
        "    bar_width_ratio: float = 0.8\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Initialize with default color map for the actual models.\"\"\"\n",
        "        self.color_map = {\n",
        "            'pgd': '#FF7043',          # Orange\n",
        "            'full_pgd': '#FFAB91',     # Light orange\n",
        "            'base_adam': '#A5D6A7',    # Green\n",
        "            'input_space_adv': '#90CAF9', # Blue\n",
        "            'decay_control': '#CE93D8'  # Purple\n",
        "        }\n",
        "\n",
        "\n",
        "def load_and_process_metrics(\n",
        "    json_path: str,\n",
        "    baseline_model: str = \"base_adam\"\n",
        ") -> Tuple[Dict[str, List[float]], List[str], List[str], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Load and process computational metrics data from JSON file.\n",
        "\n",
        "    Args:\n",
        "        json_path: Path to JSON file containing metrics\n",
        "        baseline_model: Name of model to use as baseline\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - Dictionary of relative values for each model\n",
        "        - List of model names\n",
        "        - List of metric names\n",
        "        - Dictionary of baseline values\n",
        "    \"\"\"\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Extract computational metrics section\n",
        "    metrics_data = data.get('computational_metrics', {})\n",
        "    if not metrics_data:\n",
        "        raise ValueError(\"No computational metrics found in JSON file\")\n",
        "\n",
        "    models = list(metrics_data.keys())\n",
        "    # Get metrics from first model\n",
        "    metrics = list(metrics_data[baseline_model].keys())\n",
        "\n",
        "    # Get baseline values\n",
        "    baseline_values = {\n",
        "        metric: metrics_data[baseline_model][metric]\n",
        "        for metric in metrics\n",
        "    }\n",
        "\n",
        "    # Calculate relative values\n",
        "    relative_values = {}\n",
        "    for model in models:\n",
        "        relative_values[model] = [\n",
        "            metrics_data[model][metric] / baseline_values[metric]\n",
        "            for metric in metrics\n",
        "        ]\n",
        "\n",
        "    return relative_values, models, metrics, baseline_values\n",
        "\n",
        "\n",
        "def create_bar_plot(\n",
        "    relative_values: Dict[str, List[float]],\n",
        "    models: List[str],\n",
        "    metrics: List[str],\n",
        "    config: VisualizationConfig,\n",
        "    baseline_model: str\n",
        ") -> Tuple[Figure, Axes]:\n",
        "    \"\"\"\n",
        "    Create bar plot visualization of relative metrics.\n",
        "\n",
        "    Args:\n",
        "        relative_values: Dictionary of relative values for each model\n",
        "        models: List of model names\n",
        "        metrics: List of metric names\n",
        "        config: Visualization configuration\n",
        "        baseline_model: Name of baseline model\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (Figure, Axes) for the created plot\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=config.figsize)\n",
        "\n",
        "    # Calculate bar positions\n",
        "    x = np.arange(len(metrics))\n",
        "    width = config.bar_width_ratio / len(models)\n",
        "\n",
        "    # Plot bars for each model\n",
        "    for i, model in enumerate(models):\n",
        "        positions = x + width * (i - len(models)/2 + 0.5)\n",
        "        bars = ax.bar(\n",
        "            positions,\n",
        "            relative_values[model],\n",
        "            width,\n",
        "            label=model,\n",
        "            color=config.color_map[model]\n",
        "        )\n",
        "\n",
        "        # Add value labels\n",
        "        _add_value_labels(ax, bars)\n",
        "\n",
        "    _customize_plot(\n",
        "        ax, baseline_model, metrics,\n",
        "        config.grid_alpha\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def _add_value_labels(ax: Axes, bars) -> None:\n",
        "    \"\"\"Add value labels on top of bars.\"\"\"\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(\n",
        "            bar.get_x() + bar.get_width()/2.,\n",
        "            height,\n",
        "            f'{height:.2f}x',\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            rotation=0\n",
        "        )\n",
        "\n",
        "\n",
        "def _customize_plot(\n",
        "    ax: Axes,\n",
        "    baseline_model: str,\n",
        "    metrics: List[str],\n",
        "    grid_alpha: float\n",
        ") -> None:\n",
        "    \"\"\"Apply custom styling to the plot.\"\"\"\n",
        "    # Set labels and title\n",
        "    ax.set_ylabel(f'Relative Cost ({baseline_model} = 1.0)')\n",
        "    ax.set_title('Computational Metrics Comparison')\n",
        "\n",
        "    # Configure x-axis with better metric names\n",
        "    ax.set_xticks(np.arange(len(metrics)))\n",
        "    metric_display_names = {\n",
        "        'forward_passes': 'Forward Passes',\n",
        "        'backward_passes': 'Backward Passes',\n",
        "        'total_flops': 'Total FLOPs',\n",
        "        'wall_time': 'Wall Time'\n",
        "    }\n",
        "    ax.set_xticklabels([metric_display_names.get(m, m) for m in metrics], rotation=45)\n",
        "\n",
        "    # Add baseline reference line\n",
        "    ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Configure grid\n",
        "    ax.grid(True, axis='y', linestyle='-', alpha=grid_alpha)\n",
        "    ax.set_axisbelow(True)\n",
        "\n",
        "    # Remove top and right spines\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "    # Configure legend with better model names\n",
        "    model_display_names = {\n",
        "        'pgd': 'PGD (Ours)',\n",
        "        'full_pgd': 'Full PGD',\n",
        "        'base_adam': 'Base Adam',\n",
        "        'input_space_adv': 'Input Space Adv',\n",
        "        'decay_control': 'Weight Decay'\n",
        "    }\n",
        "\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend(\n",
        "        handles,\n",
        "        [model_display_names.get(label, label) for label in labels],\n",
        "        title='Models',\n",
        "        bbox_to_anchor=(1.05, 1),\n",
        "        loc='upper left'\n",
        "    )\n",
        "\n",
        "\n",
        "def visualize_computational_metrics(\n",
        "    json_path: str,\n",
        "    baseline_model: str = \"base_adam\",\n",
        "    figsize: Tuple[int, int] = (12, 6),\n",
        "    save_path: Optional[str] = None\n",
        ") -> Tuple[Figure, Axes]:\n",
        "    \"\"\"\n",
        "    Create visualization of computational metrics comparison.\n",
        "\n",
        "    Args:\n",
        "        json_path: Path to JSON file containing metrics data\n",
        "        baseline_model: Name of model to use as baseline for comparison\n",
        "        figsize: Tuple of (width, height) for the figure\n",
        "        save_path: Optional path to save the visualization\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (Figure, Axes) for the created plot\n",
        "\n",
        "    Example:\n",
        "        >>> fig, ax = visualize_computational_metrics(\n",
        "        ...     'results.json',\n",
        "        ...     save_path='computational_metrics.png'\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    # Initialize configuration\n",
        "    config = VisualizationConfig()\n",
        "    config.figsize = figsize\n",
        "\n",
        "    # Load and process data\n",
        "    relative_values, models, metrics, _ = load_and_process_metrics(\n",
        "        json_path, baseline_model\n",
        "    )\n",
        "\n",
        "    # Create visualization\n",
        "    fig, ax = create_bar_plot(\n",
        "        relative_values, models, metrics,\n",
        "        config, baseline_model\n",
        "    )\n",
        "\n",
        "    # Save if path provided\n",
        "    if save_path:\n",
        "        plt.savefig(\n",
        "            save_path,\n",
        "            bbox_inches='tight',\n",
        "            dpi=300\n",
        "        )\n",
        "        print(f\"Saved visualization to {save_path}\")\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-cgm-gmTrG9"
      },
      "outputs": [],
      "source": [
        "def load_model(\n",
        "    file_path: Union[str, Path],\n",
        ") -> Optional[nn.Module]:\n",
        "    \"\"\"\n",
        "    Load a PyTorch model from a checkpoint file.\n",
        "\n",
        "    Supports loading from a state dictionary saved as OrderedDict in .pt format.\n",
        "    Args:\n",
        "        file_path: Path to the model checkpoint file\n",
        "\n",
        "    Returns:\n",
        "        Loaded PyTorch model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert path to Path object for better handling\n",
        "        file_path = Path(file_path)\n",
        "        model = ArithmeticNet()  # Create new model instance\n",
        "\n",
        "        # Load checkpoint (state dict)\n",
        "        state_dict = torch.load(file_path)\n",
        "        # If it's already a state dict (OrderedDict), use it directly\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        print(f\"Successfully loaded model from {file_path}\")\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from {file_path}: {str(e)}\")\n",
        "        print(\"Detailed error info:\")\n",
        "        print(f\"  - File exists: {file_path.exists()}\")\n",
        "        print(f\"  - File size: {file_path.stat().st_size if file_path.exists() else 'N/A'}\")\n",
        "        print(f\"  - Error type: {type(e).__name__}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKOMY8YOI70x"
      },
      "outputs": [],
      "source": [
        "def create_ood_comparison_plots(json_path, save_dir):\n",
        "    \"\"\"\n",
        "    Create comparison plots for different test types, showing relative performance\n",
        "    between different models.\n",
        "\n",
        "    Args:\n",
        "        json_path: Path to the JSON file containing test results\n",
        "        save_dir: Directory to save the generated plots\n",
        "    \"\"\"\n",
        "    # Read JSON file\n",
        "    with open(json_path, 'r') as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    # Define colors for each model\n",
        "    colors = {\n",
        "        'pgd': '#FF7043',          # Orange\n",
        "        'full_pgd': '#FFAB91',     # Light orange\n",
        "        'base_adam': '#A5D6A7',    # Green\n",
        "        'input_space_adv': '#90CAF9', # Blue\n",
        "        'decay_control': '#CE93D8'  # Purple\n",
        "    }\n",
        "\n",
        "    # Define display names for models and test types\n",
        "    model_display_names = {\n",
        "        'pgd': 'PGD',\n",
        "        'full_pgd': 'Full PGD',\n",
        "        'base_adam': 'Base Adam',\n",
        "        'input_space_adv': 'Input Space Adv',\n",
        "        'decay_control': 'Weight Decay'\n",
        "    }\n",
        "\n",
        "    test_types = {\n",
        "        'number_format': 'Number Format Tests',\n",
        "        'novel_operator': 'Novel Operator Tests',\n",
        "        'cross_boundary': 'Cross Boundary Tests'\n",
        "    }\n",
        "\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Create a separate plot for each test type\n",
        "    for test_type, title in test_types.items():\n",
        "        # Prepare data\n",
        "        model_names = list(results.keys())\n",
        "        recalls = [results[m][test_type]['abstention_recall'] for m in model_names]\n",
        "\n",
        "        # Convert recalls to percentages\n",
        "        recalls = [r * 100 for r in recalls]\n",
        "\n",
        "        x = np.arange(len(model_names))\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "        # Plot bars\n",
        "        bar_width = 0.7\n",
        "        bars = ax.bar(x, recalls, width=bar_width,\n",
        "                     color=[colors[name] for name in model_names])\n",
        "\n",
        "        # Configure axes\n",
        "        y_max = max(recalls)\n",
        "        ax.set_ylim(0, y_max * 1.2)  # 20% padding on top\n",
        "        ax.grid(True, which=\"major\", ls=\"-\", alpha=0.2)\n",
        "\n",
        "        # Format y-axis to show percentages\n",
        "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}%'))\n",
        "\n",
        "        # Configure labels\n",
        "        ax.set_ylabel('Abstention Recall Rate (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels([model_display_names[name] for name in model_names],\n",
        "                          fontsize=11, fontweight='bold', rotation=45, ha='right')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, val in zip(bars, recalls):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, height + (y_max * 0.01),\n",
        "                   f'{val:.1f}%',\n",
        "                   ha='center', va='bottom',\n",
        "                   fontsize=11, fontweight='bold',\n",
        "                   color='black')\n",
        "\n",
        "        # Add title\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "        # Calculate performance comparison for PGD vs others\n",
        "        pgd_val = results['pgd'][test_type]['abstention_recall'] * 100\n",
        "        other_models = [m for m in model_names if m != 'pgd']\n",
        "        best_other = max(results[m][test_type]['abstention_recall'] * 100\n",
        "                        for m in other_models)\n",
        "\n",
        "        # Calculate relative difference\n",
        "        diff_percentage = ((pgd_val - best_other) / max(best_other, 1e-9)) * 100.0\n",
        "\n",
        "        # Add performance comparison annotation\n",
        "        is_better = pgd_val > best_other\n",
        "        annotation_text = (\n",
        "            f\"+{abs(diff_percentage):.0f}% improvement\"\n",
        "            if is_better\n",
        "            else f\"{abs(diff_percentage):.0f}% lower\"\n",
        "        )\n",
        "\n",
        "        # Position annotation\n",
        "        pgd_idx = model_names.index('pgd')\n",
        "        arrow_start = pgd_idx\n",
        "        text_x = pgd_idx + 1.5\n",
        "\n",
        "        if is_better:\n",
        "            text_y = pgd_val * 1.05\n",
        "            connection_style = \"arc3,rad=0.2\"\n",
        "        else:\n",
        "            text_y = pgd_val * 0.95\n",
        "            connection_style = \"arc3,rad=-0.2\"\n",
        "\n",
        "        ax.annotate(\n",
        "            annotation_text,\n",
        "            xy=(arrow_start, pgd_val),\n",
        "            xytext=(text_x, text_y),\n",
        "            arrowprops=dict(\n",
        "                arrowstyle='->',\n",
        "                color=colors['pgd'],\n",
        "                linewidth=2,\n",
        "                connectionstyle=connection_style,\n",
        "                shrinkA=5,\n",
        "                shrinkB=5\n",
        "            ),\n",
        "            ha='left',\n",
        "            va='center',\n",
        "            color=colors['pgd'],\n",
        "            fontweight='bold',\n",
        "            fontsize=11\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        outfile = os.path.join(save_dir, f'{test_type}_recall_comparison.png')\n",
        "        plt.savefig(outfile, dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OT_e9b6AUiU"
      },
      "outputs": [],
      "source": [
        "def fishers_method(pvalues):\n",
        "    \"\"\"Implement Fisher's method for combining p-values.\"\"\"\n",
        "    valid_pvals = [p for p in pvalues if p is not None and 0 < p < 1]\n",
        "\n",
        "    if not valid_pvals:\n",
        "        return None\n",
        "\n",
        "    statistic = -2 * np.sum(np.log(valid_pvals))\n",
        "    df = 2 * len(valid_pvals)\n",
        "    combined_p = 1 - chi2.cdf(statistic, df)\n",
        "\n",
        "    return float(combined_p)\n",
        "\n",
        "def analyze_ood_test(results, test_type, models):\n",
        "    \"\"\"Analyze a specific OOD test type across all seeds.\"\"\"\n",
        "    # Extract recalls for this test type\n",
        "    model_recalls = {model: [] for model in models}\n",
        "    for result in results:\n",
        "        for model in models:\n",
        "            if model in result and test_type in result[model]:\n",
        "                recall = result[model][test_type]['abstention_recall']\n",
        "                model_recalls[model].append(recall)\n",
        "\n",
        "    # Calculate mean performance\n",
        "    model_means = {model: np.mean(recalls) for model, recalls in model_recalls.items()}\n",
        "\n",
        "    # Perform pairwise comparisons\n",
        "    comparison_pvalues = {}\n",
        "    for i, model1 in enumerate(models):\n",
        "        for j, model2 in enumerate(models):\n",
        "            if i < j:  # Only compare each pair once\n",
        "                recalls1 = model_recalls[model1]\n",
        "                recalls2 = model_recalls[model2]\n",
        "\n",
        "                if model_means[model1] > model_means[model2]:\n",
        "                    better_model = model1\n",
        "                    worse_model = model2\n",
        "                    better_recalls = recalls1\n",
        "                    worse_recalls = recalls2\n",
        "                else:\n",
        "                    better_model = model2\n",
        "                    worse_model = model1\n",
        "                    better_recalls = recalls2\n",
        "                    worse_recalls = recalls1\n",
        "\n",
        "                key = f\"{better_model}_better_than_{worse_model}\"\n",
        "\n",
        "                try:\n",
        "                    _, p_value = mannwhitneyu(better_recalls, worse_recalls, alternative='greater')\n",
        "                    comparison_pvalues[key] = [float(p_value)]\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in Mann-Whitney U test for {key}: {str(e)}\")\n",
        "                    comparison_pvalues[key] = [1.0]\n",
        "\n",
        "    # Apply Fisher's method\n",
        "    combined_results = {}\n",
        "    for comparison, pvalues in comparison_pvalues.items():\n",
        "        combined_p = fishers_method(pvalues)\n",
        "        combined_results[comparison] = {\n",
        "            'individual_pvalues': pvalues,\n",
        "            'combined_pvalue': combined_p,\n",
        "            'significant': combined_p < 0.05 if combined_p is not None else None\n",
        "        }\n",
        "\n",
        "    # Calculate statistics\n",
        "    mean_recalls_across_seeds = {}\n",
        "    for model in models:\n",
        "        values = model_recalls[model]\n",
        "        mean_recalls_across_seeds[model] = {\n",
        "            'values': [float(v) for v in values],\n",
        "            'mean': float(np.mean(values)),\n",
        "            'std': float(np.std(values))\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'fishers_method_results': combined_results,\n",
        "        'mean_recalls_across_seeds': mean_recalls_across_seeds,\n",
        "        'number_of_seeds': len(results)\n",
        "    }\n",
        "\n",
        "def combine_seed_results(results_dir):\n",
        "    \"\"\"Combine results from multiple seed files.\"\"\"\n",
        "    result_files = list(Path(results_dir).glob('*.json'))\n",
        "    all_results = []\n",
        "\n",
        "    print(f\"Found {len(result_files)} files\")\n",
        "\n",
        "    # Expected models and test types\n",
        "    models = ['base_adam', 'input_space_adv', 'decay_control', 'full_pgd', 'pgd']\n",
        "    test_types = ['number_format', 'novel_operator', 'cross_boundary']\n",
        "\n",
        "    for file_path in result_files:\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                all_results.append(data)\n",
        "                print(f\"Successfully loaded OOD results from {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "\n",
        "    if not all_results:\n",
        "        raise ValueError(\"No results found in the files\")\n",
        "\n",
        "    # Analyze each test type separately\n",
        "    final_results = {}\n",
        "    for test_type in test_types:\n",
        "        final_results[test_type] = analyze_ood_test(all_results, test_type, models)\n",
        "\n",
        "    return final_results\n",
        "\n",
        "def analyze_directory(directory_path):\n",
        "    \"\"\"Run analysis pipeline on a directory of OOD test results.\"\"\"\n",
        "    directory_path = Path(directory_path)\n",
        "    output_path = directory_path / 'fishers_analysis_ood.json'\n",
        "\n",
        "    # Run analysis\n",
        "    final_results = combine_seed_results(directory_path)\n",
        "\n",
        "    # Save results\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(final_results, f, indent=2)\n",
        "\n",
        "    # Print summary for each test type\n",
        "    for test_type, results in final_results.items():\n",
        "        print(f\"\\nFisher's Method Analysis Summary - {test_type}:\")\n",
        "        print(\"=\" * (42 + len(test_type)))\n",
        "        print(f\"Number of seeds analyzed: {results['number_of_seeds']}\")\n",
        "\n",
        "        print(f\"\\nMean Abstention Recalls Across Seeds ({test_type}):\")\n",
        "        for model, stats in results['mean_recalls_across_seeds'].items():\n",
        "            print(f\"{model}: {stats['mean']:.4f} ± {stats['std']:.4f}\")\n",
        "\n",
        "        print(f\"\\nSignificant Results for {test_type} (p < 0.05):\")\n",
        "        for comparison, comp_results in results['fishers_method_results'].items():\n",
        "            if comp_results['significant']:\n",
        "                print(f\"\\n{comparison}:\")\n",
        "                print(f\"  Combined p-value: {comp_results['combined_pvalue']:.6f} (SIGNIFICANT)\")\n",
        "                print(f\"  Individual p-values: {[f'{p:.6f}' for p in comp_results['individual_pvalues']]}\")\n",
        "\n",
        "    print(f\"\\nFull results saved to: {output_path}\")\n",
        "\n",
        "    return final_results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
